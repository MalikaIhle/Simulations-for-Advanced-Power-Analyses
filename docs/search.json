[
  {
    "objectID": "LM1.html",
    "href": "LM1.html",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\"))\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM1.html#get-some-real-data-as-starting-point",
    "href": "LM1.html#get-some-real-data-as-starting-point",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses - ideally bias-corrected - for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (‚ÄúBeat the blues‚Äù; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a ‚Äúpilot study‚Äù for our power analysis.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0‚Äì13: minimal depression\n14‚Äì19: mild depression\n20‚Äì28: moderate depression\n29‚Äì63: severe depression.\n\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample before treatment?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score before treatment was 23, corresponding to a ‚Äúmoderate depression‚Äù.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is \\(17-23=6\\)."
  },
  {
    "objectID": "LM1.html#enter-specific-values-for-the-model-parameters",
    "href": "LM1.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet‚Äôs rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{2}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value ‚Äú0‚Äù for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. ‚Äúslope parameter‚Äù) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat‚Äôs our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample ‚Äúvirtual participants‚Äù."
  },
  {
    "objectID": "LM1.html#what-is-the-effect-size-in-the-model",
    "href": "LM1.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. The standardized effect size relates the raw effect size to the variability. In the two-group example, this can be expressed as Cohen‚Äôs d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen‚Äôs d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM1.html#doing-the-power-analysis",
    "href": "LM1.html#doing-the-power-analysis",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a function called sim that returns the focal p-value. This function takes two parameters:\n\nn defines the required sample size\ntreatment_effect defines the treatment effect in the raw scale (i.e., reduction in BDI points)\n\nWe then use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\niterations <- 1000 # the number of Monte Carlo repetitions\nn <- 100 # the size of our simulated sample\n\nsim <- function(n=100, treatment_effect=-6) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  BDI <- 23 + treatment_effect*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  res <- lm(BDI ~ treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  return(p_value)\n}\n\n# now run the sim() function a 1000 times and store the p-values in a vector:\np_values <- replicate(iterations, sim(n=100))\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen‚Äôs \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into a loop that continuously increases the n.¬†We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet‚Äôs plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\nü•≥ Congratulations! You did your first power analysis by simulation. üéâ\nFor these simple models, we can also compute analytic solutions. Let‚Äôs verify our results with the pwr package - a linear regression with a single dichotomous predictor is equivalent to a t-test:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew üòÖ"
  },
  {
    "objectID": "LM1.html#safeguard-power-analysis",
    "href": "LM1.html#safeguard-power-analysis",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis (Perugini et al., 2014) that aims for the lower end of a two-sided 60% CI around the parameter of the treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the ‚Äúdivide-by-2‚Äù heuristic. (TODO: Link to kickoff presentation)\n\n\nWe can use the ci.smd function from the MBESS package to compute a CI around Cohen‚Äôs \\(d\\) that we computed for our treatment effect:\n\nlibrary(MBESS)\nci.smd(smd=-0.55, n.1=50, n.2=50, conf.level=.60)\n\n$Lower.Conf.Limit.smd\n[1] -0.7201263\n\n$smd\n[1] -0.55\n\n$Upper.Conf.Limit.smd\n[1] -0.377061\n\n\nHowever, in the simulated regression equation, we need the raw effect size - so we have to backtransform the standardized confidence limits into the original metric:\n\\[d = \\frac{M_{diff}}{SD} \\Rightarrow M_{diff} = d*SD = -0.377 * \\sqrt{117} = -4.08\\]\nAs the assumed effect is negative, we aim for the upper, i.e., the more conservative limit. After backtransformation in the raw metric, it is considerably smaller, at -4.1.\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -4.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(200, 400, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -4.1))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  200 0.448\n2  220 0.466\n3  240 0.549\n4  260 0.584\n5  280 0.646\n6  300 0.664\n7  320 0.708\n8  340 0.744\n9  360 0.790\n10 380 0.813\n11 400 0.822\n\n\nWith that more conservative effect size assumption, we would need around 380 participants, i.e.¬†190 per group."
  },
  {
    "objectID": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effects size, but rather for the smallest effect size of interest (SESOI). In this case, a non-significant result can be interpreted as ‚ÄúWe accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant‚Äù.\nWhat change of BDI scores is perceived as ‚Äúclinically important‚Äù? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al.¬†(2015) analyzed data sets where patients have been asked, after a treatment, whether they felt ‚Äúbetter‚Äù, ‚Äúthe same‚Äù or ‚Äúworse‚Äù. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms were measurably reduced in the BDI, patients still might answer ‚Äúfeels the same‚Äù, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. But the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nFor our example, let‚Äôs use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\n\n# CHANGE: we adjusted the range of probed sample sizes upwards, as the effect size now is considerably smaller\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible, as power monotonically increases with sample size. It suggests that this is simply Monte Carlo sampling error - 1000 iterations are not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow power increases monotonically with sample size, as expected."
  },
  {
    "objectID": "LM2.html",
    "href": "LM2.html",
    "title": "Linear Model 2: Multiple predictors",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\"))\nIn the first chapter on linear models, we had the simplest possible linear model: a continuous outcome variable is predicted by a single dichotomous predictor. In this chapter, we build up increasingly complex models by (a) adding a single continuous predictor and (b) modeling an interaction."
  },
  {
    "objectID": "LM2.html#get-some-real-data-as-starting-point",
    "href": "LM2.html#get-some-real-data-as-starting-point",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\nThe ‚ÄúBeat the blues‚Äù (BtheB) data set from the HSAUR R package contains pre-treatment baseline values (bdi.pre), along with multiple post-treatment values. Here we focus on the first post-treatment assessment, 2 months after the treatment (bdi.2m).\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# center the BDI baseline \n# (for better interpretability of the coefficient)\nBtheB$bdi.pre.c <- BtheB$bdi.pre - mean(BtheB$bdi.pre)\n\nl0 <- lm(bdi.2m ~ treatment, data=BtheB)\nsummary(l0)\n\n\nCall:\nlm(formula = bdi.2m ~ treatment, data = BtheB)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.467  -7.712  -1.712   7.288  28.533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      19.467      1.576  12.349   <2e-16 ***\ntreatmentBtheB   -4.755      2.153  -2.209   0.0296 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.57 on 95 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.04884,   Adjusted R-squared:  0.03882 \nF-statistic: 4.878 on 1 and 95 DF,  p-value: 0.02961\n\n# res_var = 10.57^2 = 112\n\nl1 <- lm(bdi.2m ~ bdi.pre.c + treatment, data=BtheB)\nsummary(l1)\n\n\nCall:\nlm(formula = bdi.2m ~ bdi.pre.c + treatment, data = BtheB)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.1789  -4.3869   0.2449   4.7610  24.2327 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    19.14311    1.24792  15.340  < 2e-16 ***\nbdi.pre.c       0.60289    0.07932   7.601 2.17e-11 ***\ntreatmentBtheB -3.95436    1.70666  -2.317   0.0227 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.366 on 94 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.3984 \nF-statistic: 32.78 on 2 and 94 DF,  p-value: 1.579e-11\n\n# res_var = 8.366^2 = 70\n\ncor(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 0.6142207\n\ncov(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 71.4608\n\n# cor_var = 0.614^2 = 38%\n\nTODO: Explain how we got to the reduction of the error term!\nWe assume (a) independence of both predictor variables. This is plausible, because the treatment was randomized. The pre-measurement explains 38% of the variance in the post-measurement. As this variane is unrelated to the treatment factor, it reduces the error term by 38%:\n\\(var_{err} = 117 * (1-0.38) = 72.54\\)\nThis is our new estimate of the error term."
  },
  {
    "objectID": "LM2.html#enter-specific-values-for-the-model-parameters",
    "href": "LM2.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet‚Äôs rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{1}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value ‚Äú0‚Äù for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. ‚Äúslope parameter‚Äù) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat‚Äôs our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample ‚Äúvirtual participants‚Äù."
  },
  {
    "objectID": "LM2.html#what-is-the-effect-size-in-the-model",
    "href": "LM2.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model 2: Multiple predictors",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. The standardized effect size relates the raw effect size to the variability. In the two-group example, this can be expressed as Cohen‚Äôs d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen‚Äôs d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM2.html#doing-the-power-analysis",
    "href": "LM2.html#doing-the-power-analysis",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a function called sim that returns the focal p-value. This function takes two parameters:\n\nn defines the required sample size\ntreatment_effect defines the treatment effect in the raw scale (i.e., reduction in BDI points)\n\nWe then use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\niterations <- 1000 # the number of Monte Carlo repetitions\nn <- 100 # the size of our simulated sample\n\nsim <- function(n=100, treatment_effect=-6) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  BDI <- 23 + treatment_effect*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  res <- lm(BDI ~ treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  return(p_value)\n}\n\n# now run the sim() function a 1000 times and store the p-values in a vector:\np_values <- replicate(iterations, sim(n=100))\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen‚Äôs \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM2.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM2.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into a loop that continuously increases the n.¬†We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet‚Äôs plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\nü•≥ Congratulations! You did your first power analysis by simulation. üéâ\nFor these simple models, we can also compute analytic solutions. Let‚Äôs verify our results with the pwr package - a linear regression with a single dichotomous predictor is equivalent to a t-test:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew üòÖ"
  },
  {
    "objectID": "LM2.html#safeguard-power-analysis",
    "href": "LM2.html#safeguard-power-analysis",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis (Perugini et al., 2014) that aims for the lower end of a two-sided 60% CI around the parameter of the treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the ‚Äúdivide-by-2‚Äù heuristic. (TODO: Link to kickoff presentation)\n\n\nWe can use the ci.smd function from the MBESS package to compute a CI around Cohen‚Äôs \\(d\\) that we computed for our treatment effect:\n\nlibrary(MBESS)\nci.smd(smd=-0.55, n.1=50, n.2=50, conf.level=.60)\n\n$Lower.Conf.Limit.smd\n[1] -0.7201263\n\n$smd\n[1] -0.55\n\n$Upper.Conf.Limit.smd\n[1] -0.377061\n\n\nHowever, in the simulated regression equation, we need the raw effect size - so we have to backtransform the standardized confidence limits into the original metric:\n\\[d = \\frac{M_{diff}}{SD} \\Rightarrow M_{diff} = d*SD = -0.377 * \\sqrt{117} = -4.08\\]\nAs the assumed effect is negative, we aim for the upper, i.e., the more conservative limit. After backtransformation in the raw metric, it is considerably smaller, at -4.1.\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -4.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(200, 400, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -4.1))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  200 0.448\n2  220 0.466\n3  240 0.549\n4  260 0.584\n5  280 0.646\n6  300 0.664\n7  320 0.708\n8  340 0.744\n9  360 0.790\n10 380 0.813\n11 400 0.822\n\n\nWith that more conservative effect size assumption, we would need around 380 participants, i.e.¬†190 per group."
  },
  {
    "objectID": "LM2.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM2.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effects size, but rather for the smallest effect size of interest (SESOI). In this case, a non-significant result can be interpreted as ‚ÄúWe accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant‚Äù.\nWhat change of BDI scores is perceived as ‚Äúclinically important‚Äù? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al.¬†(2015) analyzed data sets where patients have been asked, after a treatment, whether they felt ‚Äúbetter‚Äù, ‚Äúthe same‚Äù or ‚Äúworse‚Äù. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms were measurably reduced in the BDI, patients still might answer ‚Äúfeels the same‚Äù, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. But the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nFor our example, let‚Äôs use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\n\n# CHANGE: we adjusted the range of probed sample sizes upwards, as the effect size now is considerably smaller\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible, as power monotonically increases with sample size. It suggests that this is simply Monte Carlo sampling error - 1000 iterations are not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow power increases monotonically with sample size, as expected."
  },
  {
    "objectID": "optimizing_code.html",
    "href": "optimizing_code.html",
    "title": "Bonus: Optimizing R code for speed",
    "section": "",
    "text": "Code profiling means that the code execution is timed, just like you had a stopwatch. Your goal is to make your code snippet as fast as possible. RStudio has a built-in profiler that (in theory) allows to see which code line takes up the longest time. But in my experience, if the computation of each single line is very short (and the duration mostly comes from the many repetitions), it is very inaccurate (i.e., the time spent is allocated to the wrong lines). Therefore, we‚Äôll resort to the simplest way of timing code: We will measure overall execution time by wrapping our code in a system.time({ ... }) call. Longer code blocks need to be wrapped in curly braces {...}. The function returns multiple timings; the relevant number for us is the ‚Äúelapsed‚Äù time. This is also called the ‚Äúwall clock‚Äù time - the time you actually have to wait until computation finished.\n\nFirst, naive version\nHere is a first version of the power simulation code for a simple LM. Let‚Äôs see how long it takes:\n\nt0 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- c()\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    p_values <- c(p_values, summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"])\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\nt0\n\n   user  system elapsed \n  9.600   0.180   9.849 \n\nresult\n\n    n  power\n1 300 0.3446\n2 350 0.4096\n3 400 0.4950\n4 450 0.5574\n5 500 0.6070\n\n\nThis first version takes 9.849 seconds. Of course we have sampling error here as well; if you run this code multiple times, you will always get slightly different timings. But, again, we refrain from micro-optimizing in the millisecond range, so a single run is generally good enough. You should only tune your simulation in a way that it takes at least a few seconds; if you are in the millisecond range, the timings are imprecise and you won‚Äôt see speed improvements very well.\n\n\nRule 1: No growing vectors/data frames\nThis is one of the most common bottlenecks: You start with an empty vector (or even worse: data frame), and grow it by rbind-ing new rows to it in each iteration.\n\nt1 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # print(n)   # uncomment to see progress\n  \n  # CHANGE: Preallocate vector with the final size, initialize with NAs\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    \n    # CHANGE: assign resulting p-value to specific slot in vector\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  # Here we stil have a growing data.frame - but as this is only done 5 times, it does not matter.\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations)) \n}\n\n})\n\n# Combine the different timings in a data frame\ntimings <- rbind(t0[3], t1[3]) |> data.frame()\n\n# compute the absolute and relativ difference of consecutive rows:\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\n\ntimings\n\n  elapsed  diff rel_diff\n1   9.849    NA       NA\n2  10.203 0.354    0.036\n\n\nOK, this didn‚Äôt really change anything here. But in general (in particular with data frames) this is worth looking at.\n\n\nRule 2: Avoid data frames as far as possible\nUse matrizes instead of data frames wherever possible; or avoid them at all (as we do in the code below).\n\nt2 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    \n    # CHANGE: We don't need the data frame - just create the two variables\n    # in the environment and lm() takes them from there.\n    #df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment)\n    \n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n\n\nThis showed a substantial improvement of around -2.6 seconds; a relative gain of -25.9%.\n\n\nRule 3: Avoid unnecessary computations\nWhat do we actually need? In fact only the p-value for our focal predictor. But the lm function does so many more things, for example parsing the formula BDI ~ treatment.\nWe could strip away all overhead and do only the necessary steps: Fit the linear model, and retrieve the p-values (see https://stackoverflow.com/q/49732933). This needs some deeper knowledge of the functions and some google-fu. When you do this, you should definitely compare your results with the original result from the lm function and verify that they are identical!\n\nt3 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    # fit the model:\n    m <- .lm.fit(x, y)\n    \n    # compute p-values based on the residuals:\n    rss <- sum(m$residuals^2)\n    rdf <- length(y) - ncol(x)\n    resvar <- rss/rdf\n    R <- chol2inv(m$qr)\n    se <- sqrt(diag(R) * resvar)\n    ps <- 2*pt(abs(m$coef/se),rdf,lower.tail=FALSE)\n    \n    p_values[i] <- ps[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n\n\nThis step led to a massive improvement of around -6.8 seconds; a relative gain of -89.6%.\n\n\nRule 4: Use optimized packages\nFor many statistical models, there are packages optimized for speed, see for example here: https://stackoverflow.com/q/49732933\n\nlibrary(RcppArmadillo)\n\nt4 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    \n    # compute the p-value - but only for the coefficient of interest!\n    p_values[i] <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n5   0.749 -0.041   -0.052\n\n\nThis step only gave a minor -5% relative increase in speed - but as a bonus, it made our code much easier to read and shorter.\n\n\nPreparation: Wrap the simulation in a function #FIXME perhaps it could under under rule 5: Wrap up in a function and go parallel, with A and B section, because the title ‚Äòpreparation‚Äô got me a bit confused at first\nThe next step does not really change a lot: We put the simulation code into a separate function that returns the quantity of interest (in our case: the focal p-value). Different settings of the simulation parameters, such as the sample size or the effect size, can be defined as parameters of the function.\nEvery single function call sim() now gives you one simulated p-value - try it out!\nWe then use the replicate function to run the sim function many times and to store the resulting p-values in a vector. Programming the simulation in such a functional style also has the nice side effect that you do not have to pre-allocate the results vector; this is automatically done by the replicate function.\n\nlibrary(RcppArmadillo)\n\n# Wrap the code for a single simulation into a function. It returns the quantity of interest.\nsim <- function(n=100) {\n  # the \"n\" is now taken from the function parameter \"n\"\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n  mdl <- RcppArmadillo::fastLmPure(x, y)\n  p_val <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n\n  return(p_val)\n}\n\nt5 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(n=iterations, sim(n=n))\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n5   0.749 -0.041   -0.052\n6   0.885  0.136    0.182\n\n\nWhile this refactoring actually slightly increased computation time, we need this for the last, final optimization where we reap the benefits.\n\n\nRule 5: Go parallel\nBy default, R runs single-threaded. That means, a single CPU core works off all lines of code sequentially. When you optimized this single thread performance, the only way to gain more speed (except buying a faster computer) is to distribute the workload to multiple CPU cores that work in parallel. Every modern CPU comes with multiple cores (also called ‚Äúworkers‚Äù in the code); typically 4 to 8 on local computers and laptops.\nWith the use of the replicate function in the previous step, we prepared everything for an easy switch to multi-core processing. You only need to load the future.apply package, start a multi-core session with the plan command, and replace the replicate function call with future_replicate.\n\nlibrary(RcppArmadillo)\nlibrary(future.apply)\n\n# Show how many cores are available on your machine:\navailableCores()\n\nsystem \n     8 \n\n# with plan() you enter the parallel mode. Enter the number of workers (aka. CPU cores)\nplan(multisession, workers = 4)\n\nt6 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # future.seed = TRUE is needed to set seeds in all parallel processes. Then the computation is reprpducible.\n  p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3], t6[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3) |> round(2)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354     0.04\n3   7.563 -2.640    -0.26\n4   0.790 -6.773    -0.90\n5   0.749 -0.041    -0.05\n6   0.885  0.136     0.18\n7   0.722 -0.163    -0.18\n\n\nThe speed improvement seems only small - with 4 workers, one might expect that the computations only need 1/4th of the previous time. But parallel processing creates some overhead. For example, 4 separate R sessions need to be created and all packages, code (and sometimes data) need to be loaded in each session. Finally, all results must be collected and aggregated from all separate sessions. This can add up to substantial one-time costs. If your (single-core) computations only take a few seconds or less, parallel processing can even take longer.\n\n\n\n\n\n\nNote\n\n\n\nTo improve the benefits of parallel processing, make sure that each single parallel process runs as long (i.e., in an uninterrupted way) as possible. In the current example, we loop through the different n‚Äôs, and at every n, 4 parallel processes are spawned. A more efficient way would be to distribute the levels of n to separated workers which then do all replications in a row. #FIXME I am not sure I understand this paragraph\n\n\n\n\nThe final speed test: Burn your machine üî•\nLet‚Äôs see if parallel processing has an advantage when we have longer computations. We now expand the simulation by exploring a broad parameter range (n ranging from 100 to 1000) and increasing the iterations to 20,000 for more stable results. (See also: ‚ÄúBonus: How many Monte-Carlo iterations are necessary?‚Äù)\n\nlibrary(RcppArmadillo)\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\niterations <- 20000\nns <- seq(100, 1000, by=50)\nresult_single <- result_parallel <- data.frame()\n\n# single core\nt_single <- system.time({\n  for (n in ns) {\n    p_values <- replicate(n=iterations, sim(n=n))\n    result_single <- rbind(result_single, data.frame(n = n, power = sum(p_values < .005)/iterations))\n  }\n})\n\n# multi-core\nt_parallel <- system.time({\n  for (n in ns) {\n    p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n    result_parallel <- rbind(result_parallel, data.frame(n = n, power = sum(p_values < .005)/iterations))\n  }\n})\n\n# compare results\ncbind(result_single, power.parallel = result_parallel[, 2])\n\n      n   power power.parallel\n1   100 0.07230        0.07615\n2   150 0.12475        0.12750\n3   200 0.20110        0.18865\n4   250 0.26515        0.26450\n5   300 0.33965        0.33730\n6   350 0.41390        0.40815\n7   400 0.48415        0.47950\n8   450 0.54495        0.55080\n9   500 0.60765        0.60610\n10  550 0.67105        0.66795\n11  600 0.71790        0.71485\n12  650 0.76480        0.75995\n13  700 0.79890        0.80625\n14  750 0.84145        0.83660\n15  800 0.86190        0.86195\n16  850 0.89250        0.88860\n17  900 0.91200        0.90780\n18  950 0.93030        0.92690\n19 1000 0.94160        0.93840\n\nrbind(t_single, t_parallel) |> data.frame()\n\n           user.self sys.self elapsed user.child sys.child\nt_single      16.361    0.791  17.195          0         0\nt_parallel     1.148    0.063   6.762          0         0\n\n\n\n\n\nWith this optimized setup, we are running 380000 simulations in just 6.762 seconds. If you try this with the first code version, it takes 169.556 seconds.\nWith the final, parallelized version we have a 25.1x speed gain relative to the first version!\n\n\nRecap\nWe covered the most important steps for speeding up your code in R:\n\nNo growing vectors/data frames. Solution: Pre-allocate the results vector.\nAvoid data.frames. Solution: Use matrices wherever possible, or switch to data.table for more complex data structures (not covered here).\nAvoid unnecessary computations and/or switch to optimized packages that do the same computations much faster. Solution: TODO (Rfast)\nSwitch to parallel processing. Solution: If you already programmed your simulations with the replicate function, it is very easy with the future.apply package.\n\nSome steps, such as avoiding growing vectors, didn‚Äôt really help in our current example, but will help a lot in other scenarios.\nThere are many blog post showing and comparing strategies to increase R performance, e.g.:\n\nhttps://www.r-bloggers.com/2016/01/strategies-to-speedup-r-code/\nhttps://adv-r.hadley.nz/perf-improve.html\nhttps://csgillespie.github.io/efficientR/performance.html\n\n\n\n\n\n\n\nBut always remember:\n\n\n\n‚ÄúWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.‚Äù\nDonald Knuth (Structured Programming with go to Statements, ACM Journal Computing Surveys, Vol 6, No.¬†4, Dec.¬†1974. p.¬†268)\n\n\n\n\nSession Info\nThese speed measurements have been performed on a 2021 MacBook Pro with M1 processor.\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 13.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] future.apply_1.10.0      future_1.30.0            RcppArmadillo_0.11.4.3.1\n[4] prettycode_1.1.0         colorDF_0.1.7           \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       parallelly_1.34.0 rstudioapi_0.14   knitr_1.42       \n [5] magrittr_2.0.3    rlang_1.0.6       fastmap_1.1.0     globals_0.16.2   \n [9] tools_4.2.0       parallel_4.2.0    xfun_0.36         cli_3.6.0        \n[13] htmltools_0.5.4   yaml_2.3.7        digest_0.6.31     lifecycle_1.0.3  \n[17] crayon_1.5.2      purrr_1.0.1       htmlwidgets_1.6.1 vctrs_0.5.2      \n[21] codetools_0.2-18  evaluate_0.20     rmarkdown_2.20    compiler_4.2.0   \n[25] jsonlite_1.8.4    listenv_0.9.0"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  }
]