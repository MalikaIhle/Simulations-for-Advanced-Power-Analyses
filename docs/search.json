[
  {
    "objectID": "GLMM.html",
    "href": "GLMM.html",
    "title": "GLMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "Structural Equation Models (SEM)",
    "section": "",
    "text": "#install.packages(c(\"lavaan\", \"ggplot2\", \"MASS\", \"apaTables\", \"MBESS\"), dependencies = TRUE)\n\nlibrary(\"lavaan\")\nlibrary(\"MASS\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"MBESS\")\nIn this chapter, we will focus on a few rather simple Structural Equation Models (SEM). The goal is to illustrate how simulations can be used to estimate statistical power to detect a given effect in a SEM. In the context of SEMs, the focal effect may be for instance a fit index (e.g., Chi-Square, RMSEA, etc.) or a model coefficient (e.g., a regression coefficient for the association between two latent factors). In this chapter, we only focus on the latter, that is, power analyses for regression coefficients in the context of SEM. Please see the bonus chapter titled “XXX” if you’re interested in power analyses for fit indices."
  },
  {
    "objectID": "SEM.html#lets-get-some-real-data-as-starting-point",
    "href": "SEM.html#lets-get-some-real-data-as-starting-point",
    "title": "Structural Equation Models (SEM)",
    "section": "Let’s get some real data as starting point",
    "text": "Let’s get some real data as starting point\nJust like for any other simulation-based power analysis, we first need to come up with plausible estimates of the distribution of the (manifest) variables. For the sake of simplicity, let’s assume that there is a published study that measured manifestations of our two latent variables and that the corresponding data set is publicly available. For the purpose of this tutorial, we will draw on a publication by Bergh et al (2016) and the corresponding data set which has been made accessible as part of the MPsychoR package. Let’s take a look at this data set.\n\n#install.packages(\"MPsychoR\")\nlibrary(MPsychoR)\ndata(\"Bergh\")\n\n#let's take a look\nhead(Bergh)\n\n        EP    SP  HP       DP     A1       A2       A3     O1       O2       O3\n1 2.666667 3.125 1.4 2.818182 3.4375 3.600000 3.352941 2.8750 3.400000 3.176471\n2 2.666667 3.250 1.4 2.545455 2.3125 2.666667 3.117647 4.4375 3.866667 4.470588\n3 1.000000 1.625 2.7 2.000000 3.5625 4.600000 3.941176 4.2500 3.666667 3.705882\n4 2.666667 2.750 1.8 2.818182 2.7500 3.200000 3.352941 2.8750 3.400000 3.117647\n5 2.888889 3.250 2.7 3.000000 3.2500 4.200000 3.764706 3.9375 4.400000 4.294118\n6 2.000000 2.375 1.7 2.181818 3.2500 3.333333 2.941176 3.8125 3.066667 3.411765\n  gender\n1   male\n2   male\n3   male\n4   male\n5   male\n6   male\n\ntail(Bergh)\n\n          EP    SP  HP       DP     A1       A2       A3     O1       O2\n856 2.000000 2.500 1.0 2.363636 3.3125 3.800000 3.705882 3.6875 3.733333\n857 1.555556 1.875 1.1 1.818182 2.6250 3.733333 3.000000 3.3125 2.800000\n858 3.000000 2.750 1.0 1.909091 3.3125 3.533333 3.882353 4.0000 3.533333\n859 1.444444 1.250 1.0 1.000000 3.8750 3.800000 3.529412 3.9375 3.533333\n860 1.222222 1.625 1.0 2.363636 2.8750 3.600000 3.411765 2.8125 3.600000\n861 1.555556 1.750 1.0 1.090909 3.3750 4.266667 4.058824 3.3750 2.800000\n          O3 gender\n856 4.411765 female\n857 3.529412 female\n858 3.823529 female\n859 3.411765 female\n860 3.058824 female\n861 3.588235 female\n\n\nThis data set comprises 11 variables measured in 861 participants. For now, we will focus on the following measured variables:\n\nEP is a continuous variable measuring ethnic prejudice.\nSP is a continuous variable measuring sexism.\nHP is a continuous variable measuring sexual prejudice towards gays and lesbians.\nDP is a continuous variable measuring prejudice toward mentally people with disabilities.\nO1, O2, and O3 are three items measuring openness to experience.\n\nTo get an impression of this data, we can inspect means, standard deviations, and correlations of the variables we’re interested in. I am using the attach function which makes it easier to access variables of this data set throughout this tutorial without specifying the data set containing this variable again and again.\n\nattach(Bergh)\napaTables::apa.cor.table(cbind(EP, SP, HP, DP, O1, O2, O3))\n\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M    SD   1            2            3            4           \n  1. EP    1.99 0.71                                                    \n                                                                        \n  2. SP    2.11 0.68 .53**                                              \n                     [.48, .58]                                         \n                                                                        \n  3. HP    1.22 1.56 .25**        .22**                                 \n                     [.19, .32]   [.16, .28]                            \n                                                                        \n  4. DP    2.06 0.53 .53**        .53**        .24**                    \n                     [.48, .58]   [.48, .57]   [.18, .30]               \n                                                                        \n  5. O1    3.55 0.48 -.35**       -.33**       -.23**       -.30**      \n                     [-.41, -.29] [-.39, -.27] [-.30, -.17] [-.36, -.24]\n                                                                        \n  6. O2    3.49 0.47 -.36**       -.31**       -.30**       -.33**      \n                     [-.42, -.30] [-.37, -.25] [-.36, -.24] [-.39, -.27]\n                                                                        \n  7. O3    3.61 0.51 -.41**       -.33**       -.29**       -.34**      \n                     [-.46, -.35] [-.39, -.27] [-.35, -.23] [-.40, -.28]\n                                                                        \n  5          6         \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n  .66**                \n  [.62, .70]           \n                       \n  .74**      .71**     \n  [.71, .77] [.68, .75]\n                       \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p < .05. ** indicates p < .01.\n \n\ndetach(Bergh)\n\nAs we have discussed in the previous chapters, the starting point of every simulation-based power analysis is to specify the population parameters of the variables of interest. With these population parameters, we can then simulate data, for example using the mrvnorm function from the MASS package. In our example, we can estimate the population parameters from the study by Bergh et al. (2016). We start by calculating the means of the variables, rounding them generously, and storing them in a vector called means_vector.\n\nattach(Bergh)\n\n#store means\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP), mean(O1), mean(O2), mean(O3)) |> round(2)\n\n#Let's take a look\nmeans_vector\n\n[1] 1.99 2.11 1.22 2.06 3.55 3.49 3.61\n\ndetach(Bergh)\n\nWe also need the variance-covariance matrix of our variables in order to simulate data. Luckily, we can estimate this from the Bergh et al. data as well. There are two ways to do this. First, we can use the cov function to obtain the variance-covariance matrix. This matrix incorporates the variance of each variable on the diagonal, and the covariances in the remaining cells.\n\nattach(Bergh)\n\n#store covariances\ncov_mat <- cov(cbind(EP, SP, HP, DP, O1, O2, O3)) |> round(2)\n\n#Let's take a look\ncov_mat\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\ndetach(Bergh)\n\nThis works well as long as we have a data set (e.g., from a pilot study or published work) to estimate the variances and covariances. In other cases, however, we might not have access to such a data set. In this case, we might only have a correlation table that was provided in a published paper. But that’s no problem either, we can simply transform the correlations and standard deviations of the variables of interest into a variance-covariance matrix. The following chunk shows how this works by using the cor2cov function from the MBESS package.\n\nattach(Bergh)\n\n#store correlation matrix \ncor_mat <- cor(cbind(EP, SP, HP, DP, O1, O2, O3)) \n\n#store standard deviations\nsd_vector <- c(sd(EP), sd(SP), sd(HP), sd(DP), sd(O1), sd(O2), sd(O3))\n\n#transform correlations and standard deviations into variance-covariance matrix\ncov_mat2 <- MBESS::cor2cov(cor.mat = cor_mat, sd = sd_vector) |> as.data.frame() |> round(2)\n\n#Let's take a look\ncov_mat2\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\ndetach(Bergh)\n\nLet’s do a plausibility check: Did the two ways to estimate the variance-covariance matrix lead to the same results?\n\ncov_mat == cov_mat2\n\n     EP   SP   HP   DP   O1   O2   O3\nEP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nSP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nHP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nDP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO1 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO2 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO3 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nIndeed, this worked! Both procedures lead to the exact same variance-covariance matrix. Now that we have an approximation of the variance-covariance matrix, we use the mvrnorm function from the MASS package to simulate data from a multivariate normal distribution. The following code simulates n = 50 observations from the specified population.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#simulate data\nmy_first_simulated_data <- MASS::mvrnorm(n = 50, mu=means_vector, Sigma = cov_mat) |> as.data.frame()\n\n#Let's take a look\nhead(my_first_simulated_data)\n\n         EP        SP         HP        DP       O1       O2       O3\n1 4.3508412 2.0871143 -2.9424043 2.4869087 3.292205 3.887843 3.572219\n2 1.6775817 1.8457274 -1.3516800 1.1336354 4.440040 4.585372 4.437092\n3 1.4934385 1.7388850 -0.9642688 2.2226353 3.942108 3.744466 4.206545\n4 1.0374421 0.8999143 -0.5141253 0.6921223 4.194128 3.916091 4.150099\n5 2.3333823 1.9972280  1.7644258 1.9233783 3.651735 4.073427 3.944976\n6 0.8538097 1.3889903 -1.0434323 1.2248546 3.582339 4.515888 3.853789\n\n\nWe could now fit a SEM to this simulated data set and check whether the regression coefficient modelling the association between openness to experience and generalized prejudice is significant at an \\(\\alpha\\)-level of .005.\n\n#specify SEM\nmodel_sem <- \"generalized_prejudice =~ EP + DP + SP + HP\n              openness =~ O1 + O2 + O3\n              generalized_prejudice ~ openness\"\n\n#fit the SEM to the simulated data set\nfit_sem <- sem(model_sem, data = my_first_simulated_data)\n\n#display the results\nsummary(fit_sem)\n\nlavaan 0.6.14 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.592\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.081\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.707    0.166    4.270    0.000\n    SP                        0.815    0.198    4.118    0.000\n    HP                        0.609    0.523    1.165    0.244\n  openness =~                                                 \n    O1                        1.000                           \n    O2                        1.406    0.233    6.030    0.000\n    O3                        1.441    0.228    6.325    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    openness                -0.800    0.311   -2.570    0.010\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.224    0.075    2.984    0.003\n   .DP                0.126    0.039    3.194    0.001\n   .SP                0.235    0.063    3.743    0.000\n   .HP                3.368    0.680    4.954    0.000\n   .O1                0.076    0.018    4.235    0.000\n   .O2                0.090    0.026    3.477    0.001\n   .O3                0.026    0.020    1.309    0.191\n   .generlzd_prjdc    0.259    0.097    2.657    0.008\n    openness          0.099    0.033    3.005    0.003\n\n# TODO: How can we enter stnadardized regression coefficients?\n#model_sem_stand <- \"generalized_prejudice =~ EP + DP + SP + HP\n#                  agreeableness =~ A1 + A2 + A3\n#                  generalized_prejudice ~ agreeableness\n#              \"\n#fit_sem_stand <- cfa(model_sem_stand, data = Bergh, std.lv=TRUE)\n#summary(fit_sem_stand, standardized=TRUE)\n\nThe results show that in this case, the regression coefficient is -0.8 which is significant with p = 0.01. But, actually, it is not our primary interest to see whether this particular simulated data set results in an acceptable model fit. Rather, we want to know how many of a theoretically infinite number of simulations yield a significant p-value of this parameter. Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size (say, 50 observations) from the specified population and store the results of the focal test (here: the p-value of the regression coefficient) in a vector called p_values.\n\n#let's do 1000 iterations\niterations <- 1000\n\n#prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\n#sample size per iteration\nn <- 50\n\n\n#simulate data\nfor(i in 1:iterations){\n\n  simulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  \n  p_values[i] <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  \n}\n\nHow many of our 1000 virtual samples would have found a significant p-value (i.e., p < .005)?\n\n#frequency table\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  265   735 \n\n#percentage of significant results\nsum(p_values < .005)/iterations*100\n\n[1] 73.5\n\n\nOnly 74% of samples with the same size of \\(n=50\\) result in a significant p-value. We conclude that \\(n=50\\) observations seems to be insufficient, as the power with these parameters is lower than 80%."
  },
  {
    "objectID": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Structural Equation Models (SEM)",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nBut how many observations do we need to find the presumed effect with a power of 80%? Like before, we can now systematically vary certain parameters (e.g., sample size) of our simulation and see how that affects power. We could, for example, vary the sample size in a range from 50 to 200. Running these simulations typically requires quite some time for your computer.\n\n#test ns between 50 and 200\nns_sem <- seq(50, 200, by=10) \n\n#prepare empty vector to store results\nresult_sem <- data.frame()\n\n#set number of iterations\niterations_sem <- 1000\n\n#write function\nsim_sem <- function(n, model, mu, Sigma) {\n  \n\n  simulated_data <- MASS::mvrnorm(n = n, mu = mu, Sigma = Sigma) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  p_value_sem <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  return(p_value_sem)\n  \n    }\n\n\n#replicate function with varying ns\nfor (n in ns_sem) {  \n  \np_values_sem <- replicate(iterations_sem, sim_sem(n = n, model = model_mediation, mu = means_vector, Sigma = cov_mat))  \nresult_sem <- rbind(result_sem, data.frame(\n    n = n,\n    power = sum(p_values_sem < .005)/iterations_sem)\n  )\n\n}\n\nLet’s plot the results:\n\nggplot(result_sem, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 10) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis graph suggests that we need a sample size of approximately 58 to reach a power of 80% with the given population estimates."
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "GLM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LM2.html",
    "href": "LM2.html",
    "title": "Linear Model 2: Multiple predictors",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\"))\nIn the first chapter on linear models, we had the simplest possible linear model: a continuous outcome variable is predicted by a single dichotomous predictor. In this chapter, we build up increasingly complex models by (a) adding a single continuous predictor and (b) modeling an interaction."
  },
  {
    "objectID": "LM2.html#get-some-real-data-as-starting-point",
    "href": "LM2.html#get-some-real-data-as-starting-point",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\nInstead of guessing the necessary quantities - in the current case, the pre-post-correlation - let’s look at real data. The “Beat the blues” (BtheB) data set from the HSAUR R package contains pre-treatment baseline values (bdi.pre), along with multiple post-treatment values. Here we focus on the first post-treatment assessment, 2 months after the treatment (bdi.2m).\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# pre-post-correlation\ncor(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 0.6142207\n\n# cor_var = 0.614^2 = 38%\n\nWe assume independence of both predictor variables. This is plausible, because the treatment was randomized. The pre-measurement explains \\(0.614^2 = 37.7\\%\\) of the variance in the post-measurement. As this variance is unrelated to the treatment factor, it reduces the variance of the error term (which was at 117) by 38%:\n\\(var_{err} = 117 * (1-0.377) = 72.9\\)\nThis is our new estimate of the error term.\nLet’s verify our computation by computing the new linear model in our pilot data:\n\n# center the BDI baseline variable\n# (for better interpretability of the coefficient)\nBtheB$bdi.pre.c <- BtheB$bdi.pre - mean(BtheB$bdi.pre)\n\n# the original model from Chapter 1, just comparing the post-measurements\nl0 <- lm(bdi.2m ~ treatment, data=BtheB)\nsummary(l0)\n\n\nCall:\nlm(formula = bdi.2m ~ treatment, data = BtheB)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.467  -7.712  -1.712   7.288  28.533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      19.467      1.576  12.349   <2e-16 ***\ntreatmentBtheB   -4.755      2.153  -2.209   0.0296 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.57 on 95 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.04884,   Adjusted R-squared:  0.03882 \nF-statistic: 4.878 on 1 and 95 DF,  p-value: 0.02961\n\n# res_var = 10.57^2 = 112\n\nl1 <- lm(bdi.2m ~ bdi.pre.c + treatment, data=BtheB)\nsummary(l1)\n\n\nCall:\nlm(formula = bdi.2m ~ bdi.pre.c + treatment, data = BtheB)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.1789  -4.3869   0.2449   4.7610  24.2327 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    19.14311    1.24792  15.340  < 2e-16 ***\nbdi.pre.c       0.60289    0.07932   7.601 2.17e-11 ***\ntreatmentBtheB -3.95436    1.70666  -2.317   0.0227 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.366 on 94 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.3984 \nF-statistic: 32.78 on 2 and 94 DF,  p-value: 1.579e-11\n\n# res_var = 8.366^2 = 70\n\ncor(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 0.6142207\n\ncov(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 71.4608\n\n# cor_var = 0.614^2 = 38%\n\nYou can see the error variance at the bottom of each lm output: “Residual standard error: 10.57”. By squaring this value the estimated variance of the erorr term is obtained: \\(10.57^2 = 112\\), close to our assumed error variance of 117.\nIn model l1, which includes the pre-measurement, the estimated error variance is reduced to \\(8.366^2 = 70\\) - very close to the value we computed above! 🥳"
  },
  {
    "objectID": "LM2.html#doing-the-power-analysis",
    "href": "LM2.html#doing-the-power-analysis",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we build this knowledge into our sim() function. Recall that we operate on the raw BDI metric; so we need to add a regression weight for the newly included baseline predictor. Luckily, this value is totally irrelevant for our power analysis: The only relevant action was the reduction of the error term - and that depends on the pre-post-correlation. In order to obtain realistic simulated values, we simply enter the estimate from our pilot study, \\(0.6\\) (see model output from l1 above):\n\\[\\text{BDI}_{post} = 23 + 0.6*\\text{BDI}_{pre} - 6*\\text{treatment} + e\\] \\[e \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(mean=0, var=73)\\]\n(You can try it: Change the regression coefficient to any other value - even zero - and you will get identical results for the power analysis that targets the treatment effect.)\nWe again use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\n# err_var is the error variance before controlling for the baseline values\n# print = TRUE prints the model summary\nsim <- function(n=100, treatment_effect=-6, pre_post_cor = 0.614, err_var = 117, pre_post_effect = 0.6, print=TRUE) {\n  \n  # define/simulate all predictor variables\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  \n  # simulate \n  BDI_pre.c <- rnorm(n, mean=23, sd=sqrt(err_var))\n  \n  # CHANGES (compared to chapter 1):\n  # - Add the covariate as continuous predictor\n  # - Reduce the SD of the error term from sqrt(117) to sqrt(72)\n  BDI_post <- \n    23 +                          # intercept\n    pre_post_effect*BDI_pre.c +    # covariate (baseline)\n    treatment_effect*treatment +  # treatment effect\n    rnorm(n, mean=0, sd=sqrt(err_var * (1-pre_post_cor^2))) # error term\n  \n  res <- lm(BDI_post ~ BDI_pre.c + treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  \n  return(p_value)\n}\n\n\n# define all predictor and simulation variables.\niterations <- 2000\nns <- seq(90, 180, by=10) # ns are already adjusted to cover the relevant range\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n))\n  \n  result <- rbind(result, data.frame(\n      n = n,\n      power = sum(p_values < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nIn the original analysis, we needed n=180 (90 in each group) for 80% power. Including the baseline covariate (which explains \\(r^2 = 0.614^2 = 38\\%\\) of the variance in post scores) reduces that number to around n=110.\nLet’s check the plausibility of our power simulation. Borm et al (2007, p. 1237) propose a simple method how to arrive at a planned sample size when switching from a simple t-test (comparing post-treatment groups) to a model that controls for the baseline:\n\n“We propose a simple method for the sample size calculation when ANCOVA is used: multiply the number of subjects required for the t-test by (1-r^2) and add one extra subject per group.\n\nWhen we enter our assumed pre-post-correlation into that formula, we arrive a n=114 - very close to our value: \\[180 * (1 - .614^2) + 2 = 114\\]"
  },
  {
    "objectID": "LM2.html#defining-the-interaction-effect-on-the-raw-scale",
    "href": "LM2.html#defining-the-interaction-effect-on-the-raw-scale",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Defining the interaction effect on the raw scale",
    "text": "Defining the interaction effect on the raw scale\n\\[\\text{BDI}_{post} = 23 + 0.6*\\text{BDI}_{pre} - 6*\\text{treatment} + e\\]"
  },
  {
    "objectID": "LM2.html#alternative-approach-defining-the-interaction-effect-size-via-eplained-variance",
    "href": "LM2.html#alternative-approach-defining-the-interaction-effect-size-via-eplained-variance",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Alternative approach: Defining the interaction effect size via “eplained variance”",
    "text": "Alternative approach: Defining the interaction effect size via “eplained variance”\nWe can approach this question by asking: How much additional variance (\\(\\Delta R^2\\)) does the interaction term explain beyond the two main effects?\nThe explained variance in a linear model is defined as the proportion of the variance in the dependent variable that is explained by the independent variables in the model. We assumed that the (unconditional) variance of the dependent variable is 117; all unexplained variance is captured in the error term, which was 70 in the previous model l1. Hence, the explained variance of our assumed population model is:\n\\[R^2 = \\frac{117-72.9}{117} = 37.6\\%\\] We can verify this computation by simulating a single data set with very large n - this gives us (close to) population estimates of these quantities:\n\nsim(n=100000, treatment_effect=-6, pre_post_cor = 0.614, err_var = 117, pre_post_effect = 0.6, print=TRUE)\n\n\nCall:\nlm(formula = BDI_post ~ BDI_pre.c + treatment)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.041  -5.785   0.039   5.741  35.780 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 22.993903   0.069148   332.5   <2e-16 ***\nBDI_pre.c    0.601104   0.002501   240.3   <2e-16 ***\ntreatment   -5.976403   0.053928  -110.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.527 on 99997 degrees of freedom\nMultiple R-squared:  0.4131,    Adjusted R-squared:  0.4131 \nF-statistic: 3.519e+04 on 2 and 99997 DF,  p-value: < 2.2e-16\n\n\n[1] 0\n\n\nb_1 is the predicted b_3 is the"
  },
  {
    "objectID": "LM1.html",
    "href": "LM1.html",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\"))\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM1.html#get-some-real-data-as-starting-point",
    "href": "LM1.html#get-some-real-data-as-starting-point",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses - ideally bias-corrected - for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (“Beat the blues”; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a “pilot study” for our power analysis.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0–13: minimal depression\n14–19: mild depression\n20–28: moderate depression\n29–63: severe depression.\n\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample before treatment?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score before treatment was 23, corresponding to a “moderate depression”.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is \\(17-23=6\\). In the following scripts, we’ll use that value as our assumed treatment effect."
  },
  {
    "objectID": "LM1.html#enter-specific-values-for-the-model-parameters",
    "href": "LM1.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet’s rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{2}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value “0” for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. “slope parameter”) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat’s our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample “virtual participants”."
  },
  {
    "objectID": "LM1.html#what-is-the-effect-size-in-the-model",
    "href": "LM1.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nResearchers often have been trained to think in standardized effect sizes, such as Cohen’s \\(d\\), a correlation \\(r\\), or other indices such as \\(f^2\\) or partial \\(\\eta^2\\). In the simulation approach, we typical work on the raw scale of variables.\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. Defining the raw effect requires some domain knowledge - you need to know your measurement scale, and you need to know what the values (and differences between values) mean. In our example, a reduction of 6 BDI points means that the average patient moves from a moderate depression (23 points) to a mild depression (17 points). Working with raw effect sizes forces you to think about your actual data (instead of plugging in content-free default standardized effect sizes), and enables you to do plausibility checks on your simulation.\nThe standardized effect size relates the raw effect size to the unexplained error variance. In the two-group example, this can be expressed as Cohen’s d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\nThe standardized effect size always relates two components: The raw effect size (here: 6 points difference) and the error variance. Hence, you can increase the standardized effect size by (a) increasing the raw treatment effect, or (b) reducing the error variance.\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen’s d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM1.html#doing-the-power-analysis",
    "href": "LM1.html#doing-the-power-analysis",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a function called sim that returns the focal p-value. This function takes two parameters:\n\nn defines the required sample size\ntreatment_effect defines the treatment effect in the raw scale (i.e., reduction in BDI points)\n\nWe then use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\niterations <- 1000 # the number of Monte Carlo repetitions\nn <- 100 # the size of our simulated sample\n\nsim <- function(n=100, treatment_effect=-6) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  BDI <- 23 + treatment_effect*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  \n  # this lm() call should be exactly the function that you use\n  # to analyse your real data set\n  res <- lm(BDI ~ treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  return(p_value)\n}\n\n# now run the sim() function a 1000 times and store the p-values in a vector:\np_values <- replicate(iterations, sim(n=100))\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen’s \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into a loop that continuously increases the n. We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet’s plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\n🥳 Congratulations! You did your first power analysis by simulation. 🎉\nFor these simple models, we can also compute analytic solutions. Let’s verify our results with the pwr package - a linear regression with a single dichotomous predictor is equivalent to a t-test:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew 😅"
  },
  {
    "objectID": "LM1.html#safeguard-power-analysis",
    "href": "LM1.html#safeguard-power-analysis",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis (Perugini et al., 2014) that aims for the lower end of a two-sided 60% CI around the parameter of the treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the “divide-by-2” heuristic. (TODO: Link to kickoff presentation)\n\n\nWe can use the ci.smd function from the MBESS package to compute a CI around Cohen’s \\(d\\) that we computed for our treatment effect:\n\nlibrary(MBESS)\nci.smd(smd=-0.55, n.1=50, n.2=50, conf.level=.60)\n\n$Lower.Conf.Limit.smd\n[1] -0.7201263\n\n$smd\n[1] -0.55\n\n$Upper.Conf.Limit.smd\n[1] -0.377061\n\n\nHowever, in the simulated regression equation, we need the raw effect size - so we have to backtransform the standardized confidence limits into the original metric. As the assumed effect is negative, we aim for the upper, i.e., the more conservative limit. After backtransformation in the raw metric, it is considerably smaller, at -4.1:\n\\[d = \\frac{M_{diff}}{SD} \\Rightarrow M_{diff} = d*SD = -0.377 * \\sqrt{117} = -4.08\\]\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -4.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(200, 400, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -4.1))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  200 0.448\n2  220 0.466\n3  240 0.549\n4  260 0.584\n5  280 0.646\n6  300 0.664\n7  320 0.708\n8  340 0.744\n9  360 0.790\n10 380 0.813\n11 400 0.822\n\n\nWith that more conservative effect size assumption, we would need around 380 participants, i.e. 190 per group."
  },
  {
    "objectID": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model I: a single dichotomous predictor",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effect size, but rather for the smallest effect size of interest (SESOI). In this case, a non-significant result can be interpreted as “We accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant”.\nWhat change of BDI scores is perceived as “clinically important”? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al. (2015) analyzed data sets where patients have been asked, after a treatment, whether they felt “better”, “the same” or “worse”. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms were measurably reduced in the BDI, patients still might answer “feels the same”, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. But the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nFor our example, let’s use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\n\n# CHANGE: we adjusted the range of probed sample sizes upwards, as the effect size now is considerably smaller\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible, as power monotonically increases with sample size. It suggests that this is simply Monte Carlo sampling error - 1000 iterations are not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow power increases monotonically with sample size, as expected."
  },
  {
    "objectID": "optimizing_code.html",
    "href": "optimizing_code.html",
    "title": "Bonus: Optimizing R code for speed",
    "section": "",
    "text": "Code profiling means that the code execution is timed, just like you had a stopwatch. Your goal is to make your code snippet as fast as possible. RStudio has a built-in profiler that (in theory) allows to see which code line takes up the longest time. But in my experience, if the computation of each single line is very short (and the duration mostly comes from the many repetitions), it is very inaccurate (i.e., the time spent is allocated to the wrong lines). Therefore, we’ll resort to the simplest way of timing code: We will measure overall execution time by wrapping our code in a system.time({ ... }) call. Longer code blocks need to be wrapped in curly braces {...}. The function returns multiple timings; the relevant number for us is the “elapsed” time. This is also called the “wall clock” time - the time you actually have to wait until computation finished.\n\nFirst, naive version\nHere is a first version of the power simulation code for a simple LM. Let’s see how long it takes:\n\nt0 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- c()\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    p_values <- c(p_values, summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"])\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\nt0\n\n   user  system elapsed \n  9.600   0.180   9.849 \n\nresult\n\n    n  power\n1 300 0.3446\n2 350 0.4096\n3 400 0.4950\n4 450 0.5574\n5 500 0.6070\n\n\nThis first version takes 9.849 seconds. Of course we have sampling error here as well; if you run this code multiple times, you will always get slightly different timings. But, again, we refrain from micro-optimizing in the millisecond range, so a single run is generally good enough. You should only tune your simulation in a way that it takes at least a few seconds; if you are in the millisecond range, the timings are imprecise and you won’t see speed improvements very well.\n\n\nRule 1: No growing vectors/data frames\nThis is one of the most common bottlenecks: You start with an empty vector (or even worse: data frame), and grow it by rbind-ing new rows to it in each iteration.\n\nt1 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # print(n)   # uncomment to see progress\n  \n  # CHANGE: Preallocate vector with the final size, initialize with NAs\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    \n    # CHANGE: assign resulting p-value to specific slot in vector\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  # Here we stil have a growing data.frame - but as this is only done 5 times, it does not matter.\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations)) \n}\n\n})\n\n# Combine the different timings in a data frame\ntimings <- rbind(t0[3], t1[3]) |> data.frame()\n\n# compute the absolute and relativ difference of consecutive rows:\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\n\ntimings\n\n  elapsed  diff rel_diff\n1   9.849    NA       NA\n2  10.203 0.354    0.036\n\n\nOK, this didn’t really change anything here. But in general (in particular with data frames) this is worth looking at.\n\n\nRule 2: Avoid data frames as far as possible\nUse matrizes instead of data frames wherever possible; or avoid them at all (as we do in the code below).\n\nt2 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    \n    # CHANGE: We don't need the data frame - just create the two variables\n    # in the environment and lm() takes them from there.\n    #df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment)\n    \n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n\n\nThis showed a substantial improvement of around -2.6 seconds; a relative gain of -25.9%.\n\n\nRule 3: Avoid unnecessary computations\nWhat do we actually need? In fact only the p-value for our focal predictor. But the lm function does so many more things, for example parsing the formula BDI ~ treatment.\nWe could strip away all overhead and do only the necessary steps: Fit the linear model, and retrieve the p-values (see https://stackoverflow.com/q/49732933). This needs some deeper knowledge of the functions and some google-fu. When you do this, you should definitely compare your results with the original result from the lm function and verify that they are identical!\n\nt3 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    # fit the model:\n    m <- .lm.fit(x, y)\n    \n    # compute p-values based on the residuals:\n    rss <- sum(m$residuals^2)\n    rdf <- length(y) - ncol(x)\n    resvar <- rss/rdf\n    R <- chol2inv(m$qr)\n    se <- sqrt(diag(R) * resvar)\n    ps <- 2*pt(abs(m$coef/se),rdf,lower.tail=FALSE)\n    \n    p_values[i] <- ps[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n\n\nThis step led to a massive improvement of around -6.8 seconds; a relative gain of -89.6%.\n\n\nRule 4: Use optimized packages\nFor many statistical models, there are packages optimized for speed, see for example here: https://stackoverflow.com/q/49732933\n\nlibrary(RcppArmadillo)\n\nt4 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    \n    # compute the p-value - but only for the coefficient of interest!\n    p_values[i] <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n5   0.749 -0.041   -0.052\n\n\nThis step only gave a minor -5% relative increase in speed - but as a bonus, it made our code much easier to read and shorter.\n\n\nPreparation: Wrap the simulation in a function #FIXME perhaps it could under under rule 5: Wrap up in a function and go parallel, with A and B section, because the title ‘preparation’ got me a bit confused at first\nThe next step does not really change a lot: We put the simulation code into a separate function that returns the quantity of interest (in our case: the focal p-value). Different settings of the simulation parameters, such as the sample size or the effect size, can be defined as parameters of the function.\nEvery single function call sim() now gives you one simulated p-value - try it out!\nWe then use the replicate function to run the sim function many times and to store the resulting p-values in a vector. Programming the simulation in such a functional style also has the nice side effect that you do not have to pre-allocate the results vector; this is automatically done by the replicate function.\n\nlibrary(RcppArmadillo)\n\n# Wrap the code for a single simulation into a function. It returns the quantity of interest.\nsim <- function(n=100) {\n  # the \"n\" is now taken from the function parameter \"n\"\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n  mdl <- RcppArmadillo::fastLmPure(x, y)\n  p_val <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n\n  return(p_val)\n}\n\nt5 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(n=iterations, sim(n=n))\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n5   0.749 -0.041   -0.052\n6   0.885  0.136    0.182\n\n\nWhile this refactoring actually slightly increased computation time, we need this for the last, final optimization where we reap the benefits.\n\n\nRule 5: Go parallel\nBy default, R runs single-threaded. That means, a single CPU core works off all lines of code sequentially. When you optimized this single thread performance, the only way to gain more speed (except buying a faster computer) is to distribute the workload to multiple CPU cores that work in parallel. Every modern CPU comes with multiple cores (also called “workers” in the code); typically 4 to 8 on local computers and laptops.\nWith the use of the replicate function in the previous step, we prepared everything for an easy switch to multi-core processing. You only need to load the future.apply package, start a multi-core session with the plan command, and replace the replicate function call with future_replicate.\n\nlibrary(RcppArmadillo)\nlibrary(future.apply)\n\n# Show how many cores are available on your machine:\navailableCores()\n\nsystem \n     8 \n\n# with plan() you enter the parallel mode. Enter the number of workers (aka. CPU cores)\nplan(multisession, workers = 4)\n\nt6 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # future.seed = TRUE is needed to set seeds in all parallel processes. Then the computation is reprpducible.\n  p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3], t6[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3) |> round(2)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354     0.04\n3   7.563 -2.640    -0.26\n4   0.790 -6.773    -0.90\n5   0.749 -0.041    -0.05\n6   0.885  0.136     0.18\n7   0.722 -0.163    -0.18\n\n\nThe speed improvement seems only small - with 4 workers, one might expect that the computations only need 1/4th of the previous time. But parallel processing creates some overhead. For example, 4 separate R sessions need to be created and all packages, code (and sometimes data) need to be loaded in each session. Finally, all results must be collected and aggregated from all separate sessions. This can add up to substantial one-time costs. If your (single-core) computations only take a few seconds or less, parallel processing can even take longer.\n\n\n\n\n\n\nNote\n\n\n\nTo improve the benefits of parallel processing, make sure that each single parallel process runs as long (i.e., in an uninterrupted way) as possible. In the current example, we loop through the different n’s, and at every n, 4 parallel processes are spawned. A more efficient way would be to distribute the levels of n to separated workers which then do all replications in a row. #FIXME I am not sure I understand this paragraph\n\n\n\n\nThe final speed test: Burn your machine 🔥\nLet’s see if parallel processing has an advantage when we have longer computations. We now expand the simulation by exploring a broad parameter range (n ranging from 100 to 1000) and increasing the iterations to 20,000 for more stable results. (See also: “Bonus: How many Monte-Carlo iterations are necessary?”)\n\nlibrary(RcppArmadillo)\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\niterations <- 20000\nns <- seq(100, 1000, by=50)\nresult_single <- result_parallel <- data.frame()\n\n# single core\nt_single <- system.time({\n  for (n in ns) {\n    p_values <- replicate(n=iterations, sim(n=n))\n    result_single <- rbind(result_single, data.frame(n = n, power = sum(p_values < .005)/iterations))\n  }\n})\n\n# multi-core\nt_parallel <- system.time({\n  for (n in ns) {\n    p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n    result_parallel <- rbind(result_parallel, data.frame(n = n, power = sum(p_values < .005)/iterations))\n  }\n})\n\n# compare results\ncbind(result_single, power.parallel = result_parallel[, 2])\n\n      n   power power.parallel\n1   100 0.07230        0.07615\n2   150 0.12475        0.12750\n3   200 0.20110        0.18865\n4   250 0.26515        0.26450\n5   300 0.33965        0.33730\n6   350 0.41390        0.40815\n7   400 0.48415        0.47950\n8   450 0.54495        0.55080\n9   500 0.60765        0.60610\n10  550 0.67105        0.66795\n11  600 0.71790        0.71485\n12  650 0.76480        0.75995\n13  700 0.79890        0.80625\n14  750 0.84145        0.83660\n15  800 0.86190        0.86195\n16  850 0.89250        0.88860\n17  900 0.91200        0.90780\n18  950 0.93030        0.92690\n19 1000 0.94160        0.93840\n\nrbind(t_single, t_parallel) |> data.frame()\n\n           user.self sys.self elapsed user.child sys.child\nt_single      16.361    0.791  17.195          0         0\nt_parallel     1.148    0.063   6.762          0         0\n\n\n\n\n\nWith this optimized setup, we are running 380000 simulations in just 6.762 seconds. If you try this with the first code version, it takes 169.556 seconds.\nWith the final, parallelized version we have a 25.1x speed gain relative to the first version!\n\n\nRecap\nWe covered the most important steps for speeding up your code in R:\n\nNo growing vectors/data frames. Solution: Pre-allocate the results vector.\nAvoid data.frames. Solution: Use matrices wherever possible, or switch to data.table for more complex data structures (not covered here).\nAvoid unnecessary computations and/or switch to optimized packages that do the same computations much faster. Solution: TODO (Rfast)\nSwitch to parallel processing. Solution: If you already programmed your simulations with the replicate function, it is very easy with the future.apply package.\n\nSome steps, such as avoiding growing vectors, didn’t really help in our current example, but will help a lot in other scenarios.\nThere are many blog post showing and comparing strategies to increase R performance, e.g.:\n\nhttps://www.r-bloggers.com/2016/01/strategies-to-speedup-r-code/\nhttps://adv-r.hadley.nz/perf-improve.html\nhttps://csgillespie.github.io/efficientR/performance.html\n\n\n\n\n\n\n\nBut always remember:\n\n\n\n“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\nDonald Knuth (Structured Programming with go to Statements, ACM Journal Computing Surveys, Vol 6, No. 4, Dec. 1974. p. 268)\n\n\n\n\nSession Info\nThese speed measurements have been performed on a 2021 MacBook Pro with M1 processor.\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 13.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] future.apply_1.10.0      future_1.30.0            RcppArmadillo_0.11.4.3.1\n[4] prettycode_1.1.0         colorDF_0.1.7           \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       parallelly_1.34.0 rstudioapi_0.14   knitr_1.42       \n [5] magrittr_2.0.3    rlang_1.0.6       fastmap_1.1.0     globals_0.16.2   \n [9] tools_4.2.0       parallel_4.2.0    xfun_0.36         cli_3.6.0        \n[13] htmltools_0.5.4   yaml_2.3.7        digest_0.6.31     lifecycle_1.0.3  \n[17] crayon_1.5.2      purrr_1.0.1       htmlwidgets_1.6.1 vctrs_0.5.2      \n[21] codetools_0.2-18  evaluate_0.20     rmarkdown_2.20    compiler_4.2.0   \n[25] jsonlite_1.8.4    listenv_0.9.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulations for Advanced Power Analyses",
    "section": "",
    "text": "This tutorial was created by Felix Schönbrodt and Moritz Fischer, with contributions from Malika Ihle, to be part of the training offering of the Ludwig-Maximilian University Open Science Center in Munich. It was initially commissioned and funded by the University of Hamburg, Faculty of Psychology and Movement Science.\nThis book is still being developed. If you have comment to contribute to its improvement, you can submit pull requests in the respective .qmd file of the source repository by clicking on the ‘Edit this page’ and ‘Report an issue’ in the right navigation panel of each page.\nIt is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#prerequisite",
    "href": "index.html#prerequisite",
    "title": "Simulations for Advanced Power Analyses",
    "section": "Prerequisite",
    "text": "Prerequisite\nThis material follows up on the self-paced tutorial: Introduction to simulation in R which teaches how to simulate data and data analyses by writing functions in R to e.g.\n\ncheck alpha so your statistical models don’t yield more than 5% false-positive results\ncheck beta (power) for easy tests such as t-tests\nprepare a preregistration and make sure your code works\ncheck your understanding of statistics"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Simulations for Advanced Power Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nComprehensive introduction to power analyses\nPlease read Chapter 1 of the SuperpowerBook by Aaron R. Caldwell, Daniël Lakens, Chelsea M. Parlett-Pelleriti, Guy Prochilo, Frederik Aust.\nThis introduction covers sample effect sizes vs population effect sizes, how to take into account the uncertainty of the sample effect size to create a safeguard effect size to be used in power analyses, why post hoc power analyses are pointless, and why it is better to calculate the minimal detectable effect instead.\nThe rest of the Superpower book teaches how to use the superpower R package to simulate factorial designs and calculate power, which may be of great interest to you! In our tutorial, we chose to teach how to write simulation ‘by hand’ so you can understand the concept and adapt it to any of your designs.\n\n\nTutorial structure\nLet’s now start power calculations for different complex models. Here are the type of models we will cover, you can pick and choose what is relevant to you:\n\nLinear Model I: a single dichotomous predictor\n\nLinear Model 2: Multiple predictors\n\n#FIXME\n\nStructural Equation Modelling (SEM)\n\nFor each model, we will follow the structure:\n\ndefine what type of data and variables need to be simulated, i.e. their distribution, their class (e.g. factor vs numerical value)\ngenerate data based on the equation of the model (data = model + error)\nrun the statistical test, and record the relevant statistic (e.g. p-value)\nreplicate step 2 and 3 to get the distribution of the statistic of interest (e.g. p-value)\nanalyze and interpret the combined results of many simulations i.e. check for which sample size you get at a significant result in 80% of the simulations\n\n\n\nSettings\nThe following packages and settings are necessary to reproduce the output of this tutorial.\n#FIXME"
  },
  {
    "objectID": "how_many_iterations.html",
    "href": "how_many_iterations.html",
    "title": "Bonus: How many Monte-Carlo iterations are necessary?",
    "section": "",
    "text": "To find the sample size needed for a study, we have previsouly use e.g. 1000 iterations of data simulation and analysis, and varied the sample size from e.g. 100 to 1000, every 50, to find where the e.g. 80% power threshold was crossed (see e.g. https://shiny.psy.lmu.de/r-tutorials/powersim/LM.html#sample-size-planning-find-the-necessary-sample-size #FIXME relative link). But is 1000 iterations giving a precise enough result?\nUsing the optimized code, we can explore how many Monte-Carlo iterations are necessary to get stable computational results: we can re-run the same simulation with the same sample size!\nLet’s start with 1000 iterations (at n = 100, and 10 repetitions of the same power analysis):\n\nset.seed(0xBEEF)\niterations <- 1000\n\n# CHANGE: do the same sample size repeatedly, and see how much different runs deviate.\nns <- rep(100, 10)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  \n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\nresult\n\n     n power\n1  100 0.065\n2  100 0.070\n3  100 0.068\n4  100 0.066\n5  100 0.067\n6  100 0.073\n7  100 0.071\n8  100 0.070\n9  100 0.071\n10 100 0.063\n\n\nAs you can see, the power estimates show some variance, ranging from 0.063 to 0.073. This can be formalized as the Monte-Carlo error (MCE), which is define as “the standard deviation of the Monte Carlo estimator, taken across hypothetical repetitions of the simulation” (Koehler et al., 2009). With 1000 iterations (and 10 repetitions), this is:\n\nsd(result$power) |> round(4)\n\n[1] 0.0031\n\n\nWe only computed 10 repetitions of our power estimate, hence the MCE estimate is quite unstable. In the next computation, we will compute 100 repetitions of each power estimate.\nHow much do we have to increase the iterations to achieve a MCE smaller than, say, 0.005 (i.e, an SD of +/- 0.5% of the power estimate)?\nLet’s loop through increasing iterations (this takes a few minutes):\n\nlibrary(RcppArmadillo)\niterations <- seq(1000, 6000, by=1000)\n\n# let's have 100 iterations to get sufficiently stable MCE estimates\nns <- rep(100, 100)\nresult <- data.frame()\n\nfor (it in iterations) {\n\n  # print(it)  uncomment for showing the progress\n\n  for (n in ns) {\n    \n    x <- cbind(\n      rep(1, n),\n      c(rep(0, n/2), rep(1, n/2))\n    )\n    \n    p_values <- rep(NA, it)\n    \n    for (i in 1:it) {\n      y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n      \n      mdl <- RcppArmadillo::fastLmPure(x, y)\n      pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n      \n      p_values[i] <- pval[2]\n    }\n    \n    result <- rbind(result, data.frame(iterations = it, n = n, power = sum(p_values < .005)/it))\n  }\n}\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(pwr)\n\n# We can compute the exact power with the analytical solution:\nexact_power <- pwr.t.test(d = 3 / sqrt(117), sig.level = 0.005, n = 50)\n\np1 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun.data=mean_cl_normal) + ggtitle(\"Power estimate (error bars = SD)\") + geom_hline(yintercept = exact_power$power, colour = \"blue\", linetype = \"dashed\")\n\np2 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun=\"sd\", geom=\"point\") + ylab(\"MCE\") + ggtitle(\"Monte Carlo Error\")\n\np1/p2\n\n\n\n\nAs you can see, the MCE gets smaller with increasing iterations. The desired precision of MCE <= .005 can be achieved at around 3000 iterations (the dashed blue line is the exact power estimate from the analytical solution). While precision increases quickly by going from 1000 to 2000 iterations, further improvements are costly in terms of computation time. In sum, 3000 iterations seems to be a good compromise for this specific power simulation.\n\n\n\n\n\n\nNote\n\n\n\nThis choice of 3000 iterations does not necessarily generalize to other power simulations with other statistical models. But in my experience, 2000 iterations typically is a good (enough) choice. I often start with 500 iterations when exploring the parameter space (i.e., looking roughly for the range of reasonable sample sizes), and then “zoom” into this range with 2000 iterations.\n\n\nIn the lower plot, you can also see that the MCE estimate itself is a bit wiggly - we would expect a smooth curve. It suffers from meta-MCE! We could increase the precision of the MCE estimate by increasing the number of repetitions (currently at 100)."
  },
  {
    "objectID": "LMM.html",
    "href": "LMM.html",
    "title": "LMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Other_resources.html",
    "href": "Other_resources.html",
    "title": "Other resources",
    "section": "",
    "text": "https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/ (see Parts I to IV)\n“Simulation-based power analysis for regression” by Andrew Hales (Youtube video, OSF Material)"
  }
]