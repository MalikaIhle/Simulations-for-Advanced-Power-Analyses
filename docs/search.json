[
  {
    "objectID": "GLMM.html",
    "href": "GLMM.html",
    "title": "GLMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "Structural Equation Models (SEM)",
    "section": "",
    "text": "#install.packages(c(\"lavaan\", \"ggplot2\", \"MASS\", \"apaTables\", \"MBESS\"), dependencies = TRUE)\n\nlibrary(\"lavaan\")\nlibrary(\"MASS\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"MBESS\")\nIn this chapter, we will focus on a few rather simple Structural Equation Models (SEM). The goal is to illustrate how simulations can be used to estimate statistical power to detect a given effect in a SEM. In the context of SEMs, the focal effect may be for instance a fit index (e.g., Chi-Square, RMSEA, etc.) or a model coefficient (e.g., a regression coefficient for the association between two latent factors). In this chapter, we only focus on the latter, that is, power analyses for regression coefficients in the context of SEM. Please see the bonus chapter titled “XXX” if you’re interested in power analyses for fit indices."
  },
  {
    "objectID": "SEM.html#lets-get-some-real-data-as-starting-point",
    "href": "SEM.html#lets-get-some-real-data-as-starting-point",
    "title": "Structural Equation Models (SEM)",
    "section": "Let’s get some real data as starting point",
    "text": "Let’s get some real data as starting point\nJust like for any other simulation-based power analysis, we first need to come up with plausible estimates of the distribution of the (manifest) variables. For the sake of simplicity, let’s assume that there is a published study that measured manifestations of our two latent variables and that the corresponding data set is publicly available. For the purpose of this tutorial, we will draw on a publication by Bergh et al (2016) and the corresponding data set which has been made accessible as part of the MPsychoR package. Let’s take a look at this data set.\n\n#install.packages(\"MPsychoR\")\nlibrary(MPsychoR)\ndata(\"Bergh\")\n\n#let's take a look\nhead(Bergh)\n\n        EP    SP  HP       DP     A1       A2       A3     O1       O2       O3\n1 2.666667 3.125 1.4 2.818182 3.4375 3.600000 3.352941 2.8750 3.400000 3.176471\n2 2.666667 3.250 1.4 2.545455 2.3125 2.666667 3.117647 4.4375 3.866667 4.470588\n3 1.000000 1.625 2.7 2.000000 3.5625 4.600000 3.941176 4.2500 3.666667 3.705882\n4 2.666667 2.750 1.8 2.818182 2.7500 3.200000 3.352941 2.8750 3.400000 3.117647\n5 2.888889 3.250 2.7 3.000000 3.2500 4.200000 3.764706 3.9375 4.400000 4.294118\n6 2.000000 2.375 1.7 2.181818 3.2500 3.333333 2.941176 3.8125 3.066667 3.411765\n  gender\n1   male\n2   male\n3   male\n4   male\n5   male\n6   male\n\ntail(Bergh)\n\n          EP    SP  HP       DP     A1       A2       A3     O1       O2\n856 2.000000 2.500 1.0 2.363636 3.3125 3.800000 3.705882 3.6875 3.733333\n857 1.555556 1.875 1.1 1.818182 2.6250 3.733333 3.000000 3.3125 2.800000\n858 3.000000 2.750 1.0 1.909091 3.3125 3.533333 3.882353 4.0000 3.533333\n859 1.444444 1.250 1.0 1.000000 3.8750 3.800000 3.529412 3.9375 3.533333\n860 1.222222 1.625 1.0 2.363636 2.8750 3.600000 3.411765 2.8125 3.600000\n861 1.555556 1.750 1.0 1.090909 3.3750 4.266667 4.058824 3.3750 2.800000\n          O3 gender\n856 4.411765 female\n857 3.529412 female\n858 3.823529 female\n859 3.411765 female\n860 3.058824 female\n861 3.588235 female\n\n\nThis data set comprises 11 variables measured in 861 participants. For now, we will focus on the following measured variables:\n\nEP is a continuous variable measuring ethnic prejudice.\nSP is a continuous variable measuring sexism.\nHP is a continuous variable measuring sexual prejudice towards gays and lesbians.\nDP is a continuous variable measuring prejudice toward mentally people with disabilities.\nO1, O2, and O3 are three items measuring openness to experience.\n\nTo get an impression of this data, we can inspect means, standard deviations, and correlations of the variables we’re interested in. I am using the attach function which makes it easier to access variables of this data set throughout this tutorial without specifying the data set containing this variable again and again.\n\nattach(Bergh)\napaTables::apa.cor.table(cbind(EP, SP, HP, DP, O1, O2, O3))\n\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M    SD   1            2            3            4           \n  1. EP    1.99 0.71                                                    \n                                                                        \n  2. SP    2.11 0.68 .53**                                              \n                     [.48, .58]                                         \n                                                                        \n  3. HP    1.22 1.56 .25**        .22**                                 \n                     [.19, .32]   [.16, .28]                            \n                                                                        \n  4. DP    2.06 0.53 .53**        .53**        .24**                    \n                     [.48, .58]   [.48, .57]   [.18, .30]               \n                                                                        \n  5. O1    3.55 0.48 -.35**       -.33**       -.23**       -.30**      \n                     [-.41, -.29] [-.39, -.27] [-.30, -.17] [-.36, -.24]\n                                                                        \n  6. O2    3.49 0.47 -.36**       -.31**       -.30**       -.33**      \n                     [-.42, -.30] [-.37, -.25] [-.36, -.24] [-.39, -.27]\n                                                                        \n  7. O3    3.61 0.51 -.41**       -.33**       -.29**       -.34**      \n                     [-.46, -.35] [-.39, -.27] [-.35, -.23] [-.40, -.28]\n                                                                        \n  5          6         \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n                       \n  .66**                \n  [.62, .70]           \n                       \n  .74**      .71**     \n  [.71, .77] [.68, .75]\n                       \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p < .05. ** indicates p < .01.\n \n\ndetach(Bergh)\n\nAs we have discussed in the previous chapters, the starting point of every simulation-based power analysis is to specify the population parameters of the variables of interest. With these population parameters, we can then simulate data, for example using the mrvnorm function from the MASS package. In our example, we can estimate the population parameters from the study by Bergh et al. (2016). We start by calculating the means of the variables, rounding them generously, and storing them in a vector called means_vector.\n\nattach(Bergh)\n\n#store means\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP), mean(O1), mean(O2), mean(O3)) |> round(2)\n\n#Let's take a look\nmeans_vector\n\n[1] 1.99 2.11 1.22 2.06 3.55 3.49 3.61\n\ndetach(Bergh)\n\nWe also need the variance-covariance matrix of our variables in order to simulate data. Luckily, we can estimate this from the Bergh et al. data as well. There are two ways to do this. First, we can use the cov function to obtain the variance-covariance matrix. This matrix incorporates the variance of each variable on the diagonal, and the covariances in the remaining cells.\n\nattach(Bergh)\n\n#store covariances\ncov_mat <- cov(cbind(EP, SP, HP, DP, O1, O2, O3)) |> round(2)\n\n#Let's take a look\ncov_mat\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\ndetach(Bergh)\n\nThis works well as long as we have a data set (e.g., from a pilot study or published work) to estimate the variances and covariances. In other cases, however, we might not have access to such a data set. In this case, we might only have a correlation table that was provided in a published paper. But that’s no problem either, we can simply transform the correlations and standard deviations of the variables of interest into a variance-covariance matrix. The following chunk shows how this works by using the cor2cov function from the MBESS package.\n\nattach(Bergh)\n\n#store correlation matrix \ncor_mat <- cor(cbind(EP, SP, HP, DP, O1, O2, O3)) \n\n#store standard deviations\nsd_vector <- c(sd(EP), sd(SP), sd(HP), sd(DP), sd(O1), sd(O2), sd(O3))\n\n#transform correlations and standard deviations into variance-covariance matrix\ncov_mat2 <- MBESS::cor2cov(cor.mat = cor_mat, sd = sd_vector) |> as.data.frame() |> round(2)\n\n#Let's take a look\ncov_mat2\n\n      EP    SP    HP    DP    O1    O2    O3\nEP  0.51  0.26  0.28  0.20 -0.12 -0.12 -0.15\nSP  0.26  0.47  0.24  0.19 -0.11 -0.10 -0.12\nHP  0.28  0.24  2.44  0.20 -0.18 -0.22 -0.23\nDP  0.20  0.19  0.20  0.28 -0.08 -0.08 -0.09\nO1 -0.12 -0.11 -0.18 -0.08  0.23  0.15  0.18\nO2 -0.12 -0.10 -0.22 -0.08  0.15  0.22  0.17\nO3 -0.15 -0.12 -0.23 -0.09  0.18  0.17  0.26\n\ndetach(Bergh)\n\nLet’s do a plausibility check: Did the two ways to estimate the variance-covariance matrix lead to the same results?\n\ncov_mat == cov_mat2\n\n     EP   SP   HP   DP   O1   O2   O3\nEP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nSP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nHP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nDP TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO1 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO2 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nO3 TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nIndeed, this worked! Both procedures lead to the exact same variance-covariance matrix. Now that we have an approximation of the variance-covariance matrix, we use the mvrnorm function from the MASS package to simulate data from a multivariate normal distribution. The following code simulates n = 50 observations from the specified population.\n\n#Set seed to make results reproducible\nset.seed(21364)\n\n#simulate data\nmy_first_simulated_data <- MASS::mvrnorm(n = 50, mu=means_vector, Sigma = cov_mat) |> as.data.frame()\n\n#Let's take a look\nhead(my_first_simulated_data)\n\n         EP        SP         HP        DP       O1       O2       O3\n1 4.3508412 2.0871143 -2.9424043 2.4869087 3.292205 3.887843 3.572219\n2 1.6775817 1.8457274 -1.3516800 1.1336354 4.440040 4.585372 4.437092\n3 1.4934385 1.7388850 -0.9642688 2.2226353 3.942108 3.744466 4.206545\n4 1.0374421 0.8999143 -0.5141253 0.6921223 4.194128 3.916091 4.150099\n5 2.3333823 1.9972280  1.7644258 1.9233783 3.651735 4.073427 3.944976\n6 0.8538097 1.3889903 -1.0434323 1.2248546 3.582339 4.515888 3.853789\n\n\nWe could now fit a SEM to this simulated data set and check whether the regression coefficient modelling the association between openness to experience and generalized prejudice is significant at an \\(\\alpha\\)-level of .005.\n\n#specify SEM\nmodel_sem <- \"generalized_prejudice =~ EP + DP + SP + HP\n              openness =~ O1 + O2 + O3\n              generalized_prejudice ~ openness\"\n\n#fit the SEM to the simulated data set\nfit_sem <- sem(model_sem, data = my_first_simulated_data)\n\n#display the results\nsummary(fit_sem)\n\nlavaan 0.6.14 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                            50\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.592\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.081\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.707    0.166    4.270    0.000\n    SP                        0.815    0.198    4.118    0.000\n    HP                        0.609    0.523    1.165    0.244\n  openness =~                                                 \n    O1                        1.000                           \n    O2                        1.406    0.233    6.030    0.000\n    O3                        1.441    0.228    6.325    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    openness                -0.800    0.311   -2.570    0.010\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.224    0.075    2.984    0.003\n   .DP                0.126    0.039    3.194    0.001\n   .SP                0.235    0.063    3.743    0.000\n   .HP                3.368    0.680    4.954    0.000\n   .O1                0.076    0.018    4.235    0.000\n   .O2                0.090    0.026    3.477    0.001\n   .O3                0.026    0.020    1.309    0.191\n   .generlzd_prjdc    0.259    0.097    2.657    0.008\n    openness          0.099    0.033    3.005    0.003\n\n# TODO: How can we enter stnadardized regression coefficients?\n#model_sem_stand <- \"generalized_prejudice =~ EP + DP + SP + HP\n#                  agreeableness =~ A1 + A2 + A3\n#                  generalized_prejudice ~ agreeableness\n#              \"\n#fit_sem_stand <- cfa(model_sem_stand, data = Bergh, std.lv=TRUE)\n#summary(fit_sem_stand, standardized=TRUE)\n\nThe results show that in this case, the regression coefficient is -0.8 which is significant with p = 0.01. But, actually, it is not our primary interest to see whether this particular simulated data set results in an acceptable model fit. Rather, we want to know how many of a theoretically infinite number of simulations yield a significant p-value of this parameter. Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size (say, 50 observations) from the specified population and store the results of the focal test (here: the p-value of the regression coefficient) in a vector called p_values.\n\n#let's do 1000 iterations\niterations <- 1000\n\n#prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\n#sample size per iteration\nn <- 50\n\n\n#simulate data\nfor(i in 1:iterations){\n\n  simulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  \n  p_values[i] <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  \n}\n\nHow many of our 1000 virtual samples would have found a significant p-value (i.e., p < .005)?\n\n#frequency table\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  265   735 \n\n#percentage of significant results\nsum(p_values < .005)/iterations*100\n\n[1] 73.5\n\n\nOnly 74% of samples with the same size of \\(n=50\\) result in a significant p-value. We conclude that \\(n=50\\) observations seems to be insufficient, as the power with these parameters is lower than 80%."
  },
  {
    "objectID": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Structural Equation Models (SEM)",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nBut how many observations do we need to find the presumed effect with a power of 80%? Like before, we can now systematically vary certain parameters (e.g., sample size) of our simulation and see how that affects power. We could, for example, vary the sample size in a range from 50 to 200. Running these simulations typically requires quite some time for your computer.\n\n#test ns between 50 and 200\nns_sem <- seq(50, 200, by=10) \n\n#prepare empty vector to store results\nresult_sem <- data.frame()\n\n#set number of iterations\niterations_sem <- 1000\n\n#write function\nsim_sem <- function(n, model, mu, Sigma) {\n  \n\n  simulated_data <- MASS::mvrnorm(n = n, mu = mu, Sigma = Sigma) |> as.data.frame()\n  fit_sem_simulated <- sem(model_sem, data = simulated_data)\n  p_value_sem <- parameterestimates(fit_sem_simulated)[8,]$pvalue\n  return(p_value_sem)\n  \n    }\n\n\n#replicate function with varying ns\nfor (n in ns_sem) {  \n  \np_values_sem <- replicate(iterations_sem, sim_sem(n = n, model = model_mediation, mu = means_vector, Sigma = cov_mat))  \nresult_sem <- rbind(result_sem, data.frame(\n    n = n,\n    power = sum(p_values_sem < .005)/iterations_sem)\n  )\n\n}\n\nLet’s plot the results:\n\nggplot(result_sem, aes(x=n, y=power)) + geom_point() + geom_line() + scale_x_continuous(n.breaks = 10) + geom_hline(yintercept= 0.8, color = \"red\")\n\n\n\n\nThis graph suggests that we need a sample size of approximately 58 to reach a power of 80% with the given population estimates."
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "GLM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LM2.html",
    "href": "LM2.html",
    "title": "Linear Model 2: Multiple predictors",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\", \"Rfast\"))\nIn the first chapter on linear models, we had the simplest possible linear model: a continuous outcome variable is predicted by a single dichotomous predictor. In this chapter, we build up increasingly complex models by (a) adding a single continuous predictor and (b) modeling an interaction."
  },
  {
    "objectID": "LM2.html#get-some-real-data-as-starting-point",
    "href": "LM2.html#get-some-real-data-as-starting-point",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\nInstead of guessing the necessary quantities - in the current case, the pre-post-correlation - let’s look at real data. The “Beat the blues” (BtheB) data set from the HSAUR R package contains pre-treatment baseline values (bdi.pre), along with multiple post-treatment values. Here we focus on the first post-treatment assessment, 2 months after the treatment (bdi.2m).\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# pre-post-correlation\ncor(BtheB$bdi.pre, BtheB$bdi.2m, use=\"p\")\n\n[1] 0.6142207\n\n\nIn our pilot data set, the pre-post-correlation is around \\(r=.6\\). We will use this value in our simulations."
  },
  {
    "objectID": "LM2.html#update-the-sim-function",
    "href": "LM2.html#update-the-sim-function",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Update the sim() function",
    "text": "Update the sim() function\nNow we add the continuous predictor into our sim() function. In order to simulate a correlated variable, we need to slightly change the workflow in our simulation.\n\nTODO: Introduce mvnorm; explain why we do not build up the regression equation as we did before.\nThe intercept is encoded in the mean value of both BDI scores.\nWe center the \\(\\text{BDI}_{pre}\\) value for better interpretability (this will be relevant in the next section on interaction effects).\n\n\n# err_var is the error variance before controlling for the baseline values\n# print = TRUE prints the model summary\nsim2 <- function(n=100, treatment_effect=-6, pre_post_cor = 0.6, err_var = 117, print=FALSE) {\n  library(Rfast)\n  mu <- c(23, 23)\n  sigma <- matrix(c(117, pre_post_cor*sqrt(117)*sqrt(117), pre_post_cor*sqrt(117)*sqrt(117), 117), nrow=2, byrow=TRUE)\n  df <- rmvnorm(n, mu, sigma) |> data.frame()\n  names(df) <- c(\"BDI_pre\", \"BDI_post0\")\n  \n  # add treatment predictor variables\n  df$treatment <- c(rep(0, n/2), rep(1, n/2))\n  \n  # center the BDI_pre value for better interpretability\n  # TODO: no need to\n  df$BDI_pre.c <- df$BDI_pre - mean(df$BDI_pre)\n  \n  # add the treatment effect to the BDI_post variable\n  # We\n  df$BDI_post <- df$BDI_post0 + df$treatment*treatment_effect\n  \n  res <- lm(BDI_post ~ BDI_pre.c + treatment, data=df)\n  summary(res)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_value) \n}\n\nLet’s test the plausibility of the new sim() function by simulating a very large sample - this should give estimates close to the true values. We also vary the assumed pre-post-correlation. When this is 0, the results should be identical to the simpler model from Chapter 1 (except one df that we lost due to the additional predictor). When the pre-post-correlation is > 0, the error variance should be reduced (see Residual standard error at the bottom of each lm output).\n\n# without pre_post_cor, the result should be the same\nsim2(n=100000, pre_post_cor=0,   print=TRUE)\n\nLoading required package: Rcpp\n\n\nLoading required package: RcppZiggurat\n\n\n\nCall:\nlm(formula = BDI_post ~ BDI_pre.c + treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.416  -7.348   0.034   7.357  47.662 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 22.995259   0.048661 472.560   <2e-16 ***\nBDI_pre.c    0.004731   0.003170   1.492    0.136    \ntreatment   -6.078869   0.068817 -88.334   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.88 on 99997 degrees of freedom\nMultiple R-squared:  0.0724,    Adjusted R-squared:  0.07238 \nF-statistic:  3902 on 2 and 99997 DF,  p-value: < 2.2e-16\n\n# with pre_post_cor, the residual error should be reduced\nsim2(n=100000, pre_post_cor=0.6, print=TRUE)\n\n\nCall:\nlm(formula = BDI_post ~ BDI_pre.c + treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.893  -5.850   0.005   5.791  35.436 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 22.981551   0.038745   593.2   <2e-16 ***\nBDI_pre.c    0.597736   0.002529   236.4   <2e-16 ***\ntreatment   -6.015040   0.054793  -109.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.664 on 99997 degrees of freedom\nMultiple R-squared:  0.4047,    Adjusted R-squared:  0.4047 \nF-statistic: 3.399e+04 on 2 and 99997 DF,  p-value: < 2.2e-16\n\n\nIndeed, with pre_post_cor = 0 the parameter estimates are identical, and with non-zero pre-post-correlation the error term gets reduced.\n\n\n\n\n\n\nAn additional plausibility check (advanced)\n\n\n\n\n\nWe assume independence of both predictor variables (BDI_pre and treatment). This is plausible, because the treatment was randomized and therefore independent from the baseline. In this case, the variance that each predictor explains in the dependent variable is additive. The pre-measurement explains \\(r^2 = .6^2 = 36\\%\\) of the variance in the post-measurement. As this variance is unrelated to the treatment factor, it reduces the variance of the error term (which was at 117) by 36%:\n\\(\\sigma^2_{err} = 117 * (1-0.36) = 74.88\\)\nThe square root of this unexplained error variance is the Residual standard error from the lm output: \\(\\sqrt{74.88} = 8.65\\)."
  },
  {
    "objectID": "LM2.html#do-the-power-analysis",
    "href": "LM2.html#do-the-power-analysis",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Do the power analysis",
    "text": "Do the power analysis\nThe next step is the same as always: use the replicate function to repeatedly call the sim function for many iterations, and increase the simulated sample size until the desired power level is achieved.\n\n\nCode\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 2000\nns <- seq(90, 180, by=10) # ns are already adjusted to cover the relevant range\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim2(n=n, pre_post_cor = 0.6))\n  \n  result <- rbind(result, data.frame(\n      n = n,\n      power = sum(p_values < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n\n     n  power\n1   90 0.6720\n2  100 0.7225\n3  110 0.7595\n4  120 0.8220\n5  130 0.8695\n6  140 0.8920\n7  150 0.9150\n8  160 0.9350\n9  170 0.9655\n10 180 0.9650\n\n\nIn the original analysis, we needed n=180 (90 in each group) for 80% power. Including the baseline covariate (which explains \\(r^2 = .6^2 = 36\\%\\) of the variance in post scores) reduces that number to around n=115.\nLet’s check the plausibility of our power simulation. Borm et al (2007, p. 1237) propose a simple method how to arrive at a planned sample size when switching from a simple t-test (comparing post-treatment groups) to a model that controls for the baseline:\n\n“We propose a simple method for the sample size calculation when ANCOVA is used: multiply the number of subjects required for the t-test by (1-r^2) and add one extra subject per group.\n\nWhen we enter our assumed pre-post-correlation into that formula, we arrive a n=117 - very close to our value: \\[180 * (1 - .6^2) + 2 = 117\\]"
  },
  {
    "objectID": "LM2.html#thinking-visually",
    "href": "LM2.html#thinking-visually",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Thinking visually",
    "text": "Thinking visually\nArriving at plausible guesses for interaction effects can be tricky. We strongly recommend to visualize your assumed interaction effect in a plot - first drawn by hand: Do you expect a disordinal (i.e., cross-over) interaction, or an ordinal one where the effect is amplified (but not reversed) by the moderating variable?\nLook at the boundaries of your variables - how large would you expect the effect to be there?\nEvaluate the effect size in subgroups: For example, imagine a group of really severely depressed patients. And then assume that the therapy works exceptionally well for them - what would be a realistic outcome for them? Would you expect them all to be at BDI<10? Probably not. Or would a very good outcome simply be that they move to a “moderate depression”? This gives you an estimate of the upper limit of your effect size. Starting from there, you can ask: What effect size would be plausible, given your background knowledge of typical effects in your field?\nOnly when you have drawn a reasonable plot by hand (and validated that with colleagues), start to work out the parameter values that you need to enter in the regression equations in order to arrive at the desired interaction plot."
  },
  {
    "objectID": "LM2.html#do-the-power-analysis-1",
    "href": "LM2.html#do-the-power-analysis-1",
    "title": "Linear Model 2: Multiple predictors",
    "section": "Do the power analysis",
    "text": "Do the power analysis\nNow that we have our desired parameter values, we update our sim() function by:\n\nimposing the interaction effect onto our dependent variable\nAdding the interaction effect to our lm analysis model\nExtracting two p-values: We now want to compute the power both for the treatment main effect and for the interaction term.\n\n\n# CHANGE: Add interaction_effect\nsim3 <- function(n=100, treatment_effect=-6, pre_post_cor = 0.6, interaction_effect = -.2, err_var = 117, print=FALSE) {\n  library(Rfast)\n  mu <- c(23, 23)\n  sigma <- matrix(c(117, pre_post_cor*sqrt(117)*sqrt(117), pre_post_cor*sqrt(117)*sqrt(117), 117), nrow=2, byrow=TRUE)\n  df <- rmvnorm(n, mu, sigma) |> data.frame()\n  names(df) <- c(\"BDI_pre\", \"BDI_post0\")\n  \n  # add treatment predictor variables\n  df$treatment <- c(rep(0, n/2), rep(1, n/2))\n  \n  # center the BDI_pre value for better interpretability\n  df$BDI_pre.c <- df$BDI_pre - mean(df$BDI_pre)\n  \n  # CHANGE: add the treatment main effect and the interaction to the BDI_post variable\n  df$BDI_post <- df$BDI_post0 + treatment_effect*df$treatment + interaction_effect*df$treatment*df$BDI_pre.c\n  \n  # CHANGE: Add interaction effect to analysis model\n  res <- lm(BDI_post ~ BDI_pre.c * treatment, data=df)\n  \n  # CHANGE: extract both focal p-values\n  p_values <- summary(res)$coefficients[c(\"treatment\", \"BDI_pre.c:treatment\"), \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_values) \n}\n\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 400, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim3(n=n, pre_post_cor = 0.6, interaction_effect = -.2))\n  \n  # CHANGE: Analyze both sets of p-values separately\n  # The p-values for the main effect are stored in the first row,\n  # the p-values for the interaction effect are stored in the 2nd row\n  \n  result <- rbind(result, data.frame(\n      n = n,\n      power_treatment = sum(p_values[1, ] < .005)/iterations,\n      power_interaction = sum(p_values[2, ] < .005)/iterations\n    )\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\n\n\n     n power_treatment power_interaction\n1  100           0.704             0.060\n2  120           0.825             0.071\n3  140           0.899             0.077\n4  160           0.925             0.118\n5  180           0.954             0.095\n6  200           0.989             0.129\n7  220           0.993             0.162\n8  240           0.991             0.161\n9  260           0.995             0.200\n10 280           1.000             0.229\n11 300           0.999             0.227\n12 320           1.000             0.281\n13 340           1.000             0.308\n14 360           1.000             0.354\n15 380           1.000             0.324\n16 400           1.000             0.372\n\n\nWhile the power for detecting the main effect quickly approaches 100%, even 400 participants are by far not enough to detect the interaction effect reliably. (In particular when you recall that we set the assumed interaction effect to the largest plausible value)."
  },
  {
    "objectID": "LM1.html",
    "href": "LM1.html",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\", \"MBESS\"))\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM1.html#get-some-real-data-as-starting-point",
    "href": "LM1.html#get-some-real-data-as-starting-point",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses - ideally bias-corrected - for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (“Beat the blues”; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a “pilot study” for our power analysis.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0–13: minimal depression\n14–19: mild depression\n20–28: moderate depression\n29–63: severe depression.\n\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample before treatment?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score before treatment was 23, corresponding to a “moderate depression”.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is \\(17-23=6\\). In the following scripts, we’ll use that value as our assumed treatment effect."
  },
  {
    "objectID": "LM1.html#enter-specific-values-for-the-model-parameters",
    "href": "LM1.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet’s rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{2}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value “0” for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. “slope parameter”) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat’s our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample “virtual participants”."
  },
  {
    "objectID": "LM1.html#what-is-the-effect-size-in-the-model",
    "href": "LM1.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nResearchers often have been trained to think in standardized effect sizes, such as Cohen’s \\(d\\), a correlation \\(r\\), or other indices such as \\(f^2\\) or partial \\(\\eta^2\\). In the simulation approach, we typical work on the raw scale of variables.\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. Defining the raw effect requires some domain knowledge - you need to know your measurement scale, and you need to know what the values (and differences between values) mean. In our example, a reduction of 6 BDI points means that the average patient moves from a moderate depression (23 points) to a mild depression (17 points). Working with raw effect sizes forces you to think about your actual data (instead of plugging in content-free default standardized effect sizes), and enables you to do plausibility checks on your simulation.\nThe standardized effect size relates the raw effect size to the unexplained error variance. In the two-group example, this can be expressed as Cohen’s d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\nThe standardized effect size always relates two components: The raw effect size (here: 6 points difference) and the error variance. Hence, you can increase the standardized effect size by (a) increasing the raw treatment effect, or (b) reducing the error variance.\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen’s d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM1.html#doing-the-power-analysis",
    "href": "LM1.html#doing-the-power-analysis",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a function called sim. We coded the function to either return the focal p-value (as default) or to print a model summary (helpful for debugging and testing the function). This function takes two parameters:\n\nn defines the required sample size\ntreatment_effect defines the treatment effect in the raw scale (i.e., reduction in BDI points)\n\nWe then use the replicate function to repeatedly call the sim function for 1000 iterations.\n\nset.seed(0xBEEF)\n\niterations <- 1000 # the number of Monte Carlo repetitions\nn <- 100 # the size of our simulated sample\n\nsim1 <- function(n=100, treatment_effect=-6, print=FALSE) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  BDI <- 23 + treatment_effect*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  \n  # this lm() call should be exactly the function that you use\n  # to analyse your real data set\n  res <- lm(BDI ~ treatment)\n  p_value <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  \n  if (print==TRUE) print(summary(res))\n  else return(p_value)\n}\n\n# now run the sim() function a 1000 times and store the p-values in a vector:\np_values <- replicate(iterations, sim1(n=100))\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen’s \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM1.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into a loop that continuously increases the n. We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim1(n=n))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet’s plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\n🥳 Congratulations! You did your first power analysis by simulation. 🎉\nFor these simple models, we can also compute analytic solutions. Let’s verify our results with the pwr package - a linear regression with a single dichotomous predictor is equivalent to a t-test:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew 😅"
  },
  {
    "objectID": "LM1.html#safeguard-power-analysis",
    "href": "LM1.html#safeguard-power-analysis",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis (Perugini et al., 2014) that aims for the lower end of a two-sided 60% CI around the parameter of the treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the “divide-by-2” heuristic. (TODO: Link to kickoff presentation)\n\n\nWe can use the ci.smd function from the MBESS package to compute a CI around Cohen’s \\(d\\) that we computed for our treatment effect:\n\nlibrary(MBESS)\nci.smd(smd=-0.55, n.1=50, n.2=50, conf.level=.60)\n\n$Lower.Conf.Limit.smd\n[1] -0.7201263\n\n$smd\n[1] -0.55\n\n$Upper.Conf.Limit.smd\n[1] -0.377061\n\n\nHowever, in the simulated regression equation, we need the raw effect size - so we have to backtransform the standardized confidence limits into the original metric. As the assumed effect is negative, we aim for the upper, i.e., the more conservative limit. After backtransformation in the raw metric, it is considerably smaller, at -4.1:\n\\[d = \\frac{M_{diff}}{SD} \\Rightarrow M_{diff} = d*SD = -0.377 * \\sqrt{117} = -4.08\\]\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -4.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(200, 400, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # loop through elements of the vector \"ns\"\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -4.1))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  200 0.448\n2  220 0.466\n3  240 0.549\n4  260 0.584\n5  280 0.646\n6  300 0.664\n7  320 0.708\n8  340 0.744\n9  360 0.790\n10 380 0.813\n11 400 0.822\n\n\nWith that more conservative effect size assumption, we would need around 380 participants, i.e. 190 per group."
  },
  {
    "objectID": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM1.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model 1: A single dichotomous predictor",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effect size, but rather for the smallest effect size of interest (SESOI). In this case, a non-significant result can be interpreted as “We accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant”.\nWhat change of BDI scores is perceived as “clinically important”? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al. (2015) analyzed data sets where patients have been asked, after a treatment, whether they felt “better”, “the same” or “worse”. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms were measurably reduced in the BDI, patients still might answer “feels the same”, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. But the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nFor our example, let’s use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\n\n# CHANGE: we adjusted the range of probed sample sizes upwards, as the effect size now is considerably smaller\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible, as power monotonically increases with sample size. It suggests that this is simply Monte Carlo sampling error - 1000 iterations are not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(iterations, sim1(n=n, treatment_effect = -3))\n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow power increases monotonically with sample size, as expected."
  },
  {
    "objectID": "optimizing_code.html",
    "href": "optimizing_code.html",
    "title": "Bonus: Optimizing R code for speed",
    "section": "",
    "text": "Optimizing code for speed can be an art - and you get lost and spend/waste hours by micro-optimizing some milliseconds. But the Pareto principle applies here: with 20% effort, you can have quick and substantial gains.\nCode profiling means that the code execution is timed, just like you had a stopwatch. Your goal is to make your code snippet as fast as possible. RStudio has a built-in profiler that (in theory) allows to see which code line takes up the longest time. But in my experience, if the computation of each single line is very short (and the duration mostly comes from the many repetitions), it is very inaccurate (i.e., the time spent is allocated to the wrong lines). Therefore, we’ll resort to the simplest way of timing code: We will measure overall execution time by wrapping our code in a system.time({ ... }) call. Longer code blocks need to be wrapped in curly braces {...}. The function returns multiple timings; the relevant number for us is the “elapsed” time. This is also called the “wall clock” time - the time you actually have to wait until computation finished."
  },
  {
    "objectID": "optimizing_code.html#preparation-wrap-the-simulation-in-a-function",
    "href": "optimizing_code.html#preparation-wrap-the-simulation-in-a-function",
    "title": "Bonus: Optimizing R code for speed",
    "section": "Preparation: Wrap the simulation in a function",
    "text": "Preparation: Wrap the simulation in a function\nThe first step does not really change a lot: We put the simulation code into a separate function that returns the quantity of interest (in our case: the focal p-value). Different settings of the simulation parameters, such as the sample size or the effect size, can be defined as parameters of the function.\nEvery single function call sim() now gives you one simulated p-value - try it out!\nWe then use the replicate function to run the sim function many times and to store the resulting p-values in a vector. Programming the simulation in such a functional style also has the nice side effect that you do not have to pre-allocate the results vector; this is automatically done by the replicate function.\n\nlibrary(RcppArmadillo)\n\n# Wrap the code for a single simulation into a function. It returns the quantity of interest.\nsim <- function(n=100) {\n  # the \"n\" is now taken from the function parameter \"n\"\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n  mdl <- RcppArmadillo::fastLmPure(x, y)\n  p_val <- 2*pt(abs(mdl$coefficients[2]/mdl$stderr[2]), mdl$df.residual, lower.tail=FALSE)\n\n  return(p_val)\n}\n\nt5 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- replicate(n=iterations, sim(n=n))\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354    0.036\n3   7.563 -2.640   -0.259\n4   0.790 -6.773   -0.896\n5   0.749 -0.041   -0.052\n6   0.885  0.136    0.182\n\n\nWhile this refactoring actually slightly increased computation time, we need this for the last, final optimization where we reap the benefits."
  },
  {
    "objectID": "optimizing_code.html#run-on-multiple-cores",
    "href": "optimizing_code.html#run-on-multiple-cores",
    "title": "Bonus: Optimizing R code for speed",
    "section": "Run on multiple cores",
    "text": "Run on multiple cores\nWith the use of the replicate function in the previous step, we prepared everything for an easy switch to multi-core processing. You only need to load the future.apply package, start a multi-core session with the plan command, and replace the replicate function call with future_replicate.\n\nlibrary(RcppArmadillo)\nlibrary(future.apply)\n\n# Show how many cores are available on your machine:\navailableCores()\n\nsystem \n     8 \n\n# with plan() you enter the parallel mode. Enter the number of workers (aka. CPU cores)\nplan(multisession, workers = 4)\n\nt6 <- system.time({\n\niterations <- 5000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # future.seed = TRUE is needed to set seeds in all parallel processes. Then the computation is reprpducible.\n  p_values <- future_replicate(n=iterations, sim(n=n), future.seed = TRUE)\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0[3], t1[3], t2[3], t3[3], t4[3], t5[3], t6[3]) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1]) |> round(3) |> round(2)\ntimings\n\n  elapsed   diff rel_diff\n1   9.849     NA       NA\n2  10.203  0.354     0.04\n3   7.563 -2.640    -0.26\n4   0.790 -6.773    -0.90\n5   0.749 -0.041    -0.05\n6   0.885  0.136     0.18\n7   0.722 -0.163    -0.18\n\n\nThe speed improvement seems only small - with 4 workers, one might expect that the computations only need 1/4th of the previous time. But parallel processing creates some overhead. For example, 4 separate R sessions need to be created and all packages, code (and sometimes data) need to be loaded in each session. Finally, all results must be collected and aggregated from all separate sessions. This can add up to substantial one-time costs. If your (single-core) computations only take a few seconds or less, parallel processing can even take longer.\n\n\n\n\n\n\nNote\n\n\n\nTo improve the benefits of parallel processing, make sure that each single parallel process runs as long (i.e., in an uninterrupted way) as possible."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulations for Advanced Power Analyses",
    "section": "",
    "text": "This tutorial was created by Felix Schönbrodt and Moritz Fischer, with contributions from Malika Ihle, to be part of the training offering of the Ludwig-Maximilian University Open Science Center in Munich. It was initially commissioned and funded by the University of Hamburg, Faculty of Psychology and Movement Science.\nThis book is still being developed. If you have comment to contribute to its improvement, you can submit pull requests in the respective .qmd file of the source repository by clicking on the ‘Edit this page’ and ‘Report an issue’ in the right navigation panel of each page.\nIt is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#prerequisite",
    "href": "index.html#prerequisite",
    "title": "Simulations for Advanced Power Analyses",
    "section": "Prerequisite",
    "text": "Prerequisite\nThis material follows up on the self-paced tutorial: Introduction to simulation in R which teaches how to simulate data and data analyses by writing functions in R to e.g.\n\ncheck alpha so your statistical models don’t yield more than 5% false-positive results\ncheck beta (power) for easy tests such as t-tests\nprepare a preregistration and make sure your code works\ncheck your understanding of statistics"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Simulations for Advanced Power Analyses",
    "section": "Introduction",
    "text": "Introduction\n\nComprehensive introduction to power analyses\nPlease read Chapter 1 of the SuperpowerBook by Aaron R. Caldwell, Daniël Lakens, Chelsea M. Parlett-Pelleriti, Guy Prochilo, Frederik Aust.\nThis introduction covers sample effect sizes vs population effect sizes, how to take into account the uncertainty of the sample effect size to create a safeguard effect size to be used in power analyses, why post hoc power analyses are pointless, and why it is better to calculate the minimal detectable effect instead.\nThe rest of the Superpower book teaches how to use the superpower R package to simulate factorial designs and calculate power, which may be of great interest to you! In our tutorial, we chose to teach how to write simulation ‘by hand’ so you can understand the concept and adapt it to any of your designs.\n\n\nTutorial structure\nLet’s now start power calculations for different complex models. Here are the type of models we will cover, you can pick and choose what is relevant to you:\n\nLinear Model I: a single dichotomous predictor\n\nLinear Model 2: Multiple predictors\n\n#FIXME\n\nStructural Equation Modelling (SEM)\n\nThe three chapters on linear models should be worked through in this order, as they built upon each other.\nFor each model, we will follow the structure:\n\ndefine what type of data and variables need to be simulated, i.e. their distribution, their class (e.g. factor vs numerical value)\ngenerate data based on the equation of the model (data = model + error)\nrun the statistical test, and record the relevant statistic (e.g. p-value)\nreplicate step 2 and 3 to get the distribution of the statistic of interest (e.g. p-value)\nanalyze and interpret the combined results of many simulations i.e. check for which sample size you get at a significant result in 80% of the simulations\n\n\n\nSettings\nThe following packages and settings are necessary to reproduce the output of this tutorial.\n#FIXME"
  },
  {
    "objectID": "how_many_iterations.html",
    "href": "how_many_iterations.html",
    "title": "Bonus: How many Monte-Carlo iterations are necessary?",
    "section": "",
    "text": "To find the sample size needed for a study, we have previsouly use e.g. 1000 iterations of data simulation and analysis, and varied the sample size from e.g. 100 to 1000, every 50, to find where the e.g. 80% power threshold was crossed (see e.g. https://shiny.psy.lmu.de/r-tutorials/powersim/LM.html#sample-size-planning-find-the-necessary-sample-size #FIXME relative link). But is 1000 iterations giving a precise enough result?\nUsing the optimized code, we can explore how many Monte-Carlo iterations are necessary to get stable computational results: we can re-run the same simulation with the same sample size!\nLet’s start with 1000 iterations (at n = 100, and 10 repetitions of the same power analysis):\n\nset.seed(0xBEEF)\niterations <- 1000\n\n# CHANGE: do the same sample size repeatedly, and see how much different runs deviate.\nns <- rep(100, 10)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  \n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\nresult\n\n     n power\n1  100 0.065\n2  100 0.070\n3  100 0.068\n4  100 0.066\n5  100 0.067\n6  100 0.073\n7  100 0.071\n8  100 0.070\n9  100 0.071\n10 100 0.063\n\n\nAs you can see, the power estimates show some variance, ranging from 0.063 to 0.073. This can be formalized as the Monte-Carlo error (MCE), which is define as “the standard deviation of the Monte Carlo estimator, taken across hypothetical repetitions of the simulation” (Koehler et al., 2009). With 1000 iterations (and 10 repetitions), this is:\n\nsd(result$power) |> round(4)\n\n[1] 0.0031\n\n\nWe only computed 10 repetitions of our power estimate, hence the MCE estimate is quite unstable. In the next computation, we will compute 100 repetitions of each power estimate.\nHow much do we have to increase the iterations to achieve a MCE smaller than, say, 0.005 (i.e, an SD of +/- 0.5% of the power estimate)?\nLet’s loop through increasing iterations (this takes a few minutes):\n\nlibrary(RcppArmadillo)\niterations <- seq(1000, 6000, by=1000)\n\n# let's have 100 iterations to get sufficiently stable MCE estimates\nns <- rep(100, 100)\nresult <- data.frame()\n\nfor (it in iterations) {\n\n  # print(it)  uncomment for showing the progress\n\n  for (n in ns) {\n    \n    x <- cbind(\n      rep(1, n),\n      c(rep(0, n/2), rep(1, n/2))\n    )\n    \n    p_values <- rep(NA, it)\n    \n    for (i in 1:it) {\n      y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n      \n      mdl <- RcppArmadillo::fastLmPure(x, y)\n      pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n      \n      p_values[i] <- pval[2]\n    }\n    \n    result <- rbind(result, data.frame(iterations = it, n = n, power = sum(p_values < .005)/it))\n  }\n}\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(pwr)\n\n# We can compute the exact power with the analytical solution:\nexact_power <- pwr.t.test(d = 3 / sqrt(117), sig.level = 0.005, n = 50)\n\np1 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun.data=mean_cl_normal) + ggtitle(\"Power estimate (error bars = SD)\") + geom_hline(yintercept = exact_power$power, colour = \"blue\", linetype = \"dashed\")\n\np2 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun=\"sd\", geom=\"point\") + ylab(\"MCE\") + ggtitle(\"Monte Carlo Error\")\n\np1/p2\n\n\n\n\nAs you can see, the MCE gets smaller with increasing iterations. The desired precision of MCE <= .005 can be achieved at around 3000 iterations (the dashed blue line is the exact power estimate from the analytical solution). While precision increases quickly by going from 1000 to 2000 iterations, further improvements are costly in terms of computation time. In sum, 3000 iterations seems to be a good compromise for this specific power simulation.\n\n\n\n\n\n\nNote\n\n\n\nThis choice of 3000 iterations does not necessarily generalize to other power simulations with other statistical models. But in my experience, 2000 iterations typically is a good (enough) choice. I often start with 500 iterations when exploring the parameter space (i.e., looking roughly for the range of reasonable sample sizes), and then “zoom” into this range with 2000 iterations.\n\n\nIn the lower plot, you can also see that the MCE estimate itself is a bit wiggly - we would expect a smooth curve. It suffers from meta-MCE! We could increase the precision of the MCE estimate by increasing the number of repetitions (currently at 100)."
  },
  {
    "objectID": "LMM.html",
    "href": "LMM.html",
    "title": "Linear Mixed Models",
    "section": "",
    "text": "Furthermore, we’ll use the techniques described in the Chapter “Bonus: Optimizing R code for speed”. If you wonder about the unknown commands in the code, please read the bonus chapter to learn how to considerably speed up your code!"
  },
  {
    "objectID": "Regression_to_mean.html",
    "href": "Regression_to_mean.html",
    "title": "Regression to the mean in pre-post-designs",
    "section": "",
    "text": "\\[BDI_{post} = b_0 + 1*BDI_{pre} + e\\]\n\\[BDI_{post} = b_0 + b_1*BDI_{pre} + b_2*treatment + e\\]\nHence, the pre value simply is carried forward to the post value. We add random error, as participants of course go somewhat up and down, but there is no systematic trend.\n\n  library(Rfast)\n  n <- 10000\n  pre_post_cor <- 0.9\n  mu <- c(23, 23)\n  sigma <- matrix(c(117, pre_post_cor*sqrt(117)*sqrt(117), pre_post_cor*sqrt(117)*sqrt(117), 117), nrow=2, byrow=TRUE)\n  BDI <- rmvnorm(n, mu, sigma) |> data.frame()\n  \n  \n  xi <- rnorm(n, mean=23, sd=sqrt(117))\n  pre <-  1*xi + rnorm(n, mean=0, sd=2)\n  post <- 1*xi + rnorm(n, mean=0, sd=2)\n  \n  cor(pre, post)\n\n[1] 0.9661545\n\n  colMeans(BDI)\n\n      X1       X2 \n22.92187 22.95114 \n\n  var(BDI)\n\n         X1       X2\nX1 118.0560 106.2575\nX2 106.2575 118.1335\n\n  names(BDI) <- c(\"pre\", \"post\")\n  BDI$id <- 1:nrow(BDI)\n  summary(lm(post~pre, BDI))\n\n\nCall:\nlm(formula = post ~ pre, data = BDI)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0754  -3.2091   0.0313   3.2705  19.4456 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.320078   0.110740   20.95   <2e-16 ***\npre         0.900060   0.004366  206.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.743 on 9998 degrees of freedom\nMultiple R-squared:  0.8096,    Adjusted R-squared:  0.8096 \nF-statistic: 4.251e+04 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n  library(tidyr)\n  library(ggplot2)\n  BDI_long <- pivot_longer(BDI, c(pre, post), names_to = \"time\")\n  ggplot(BDI_long, aes(x=time, y=value, group=id)) + geom_line()\n\n\n\n  # typically RTM pattern: The pre-post difference is\n  BDI$diff <- BDI$post-BDI$pre\n  BDI$absdiff <- abs(BDI$post-BDI$pre)\n  hist(BDI$diff)\n\n\n\n  mean(BDI$diff)\n\n[1] 0.02927231\n\n  summary(lm(diff~pre, BDI))\n\n\nCall:\nlm(formula = diff ~ pre, data = BDI)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0754  -3.2091   0.0313   3.2705  19.4456 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.320078   0.110740   20.95   <2e-16 ***\npre         -0.099940   0.004366  -22.89   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.743 on 9998 degrees of freedom\nMultiple R-squared:  0.04981,   Adjusted R-squared:  0.04971 \nF-statistic: 524.1 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n  ggplot(BDI, aes(x=pre, y=absdiff)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n  ggplot(BDI, aes(x=pre, y=post)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n  BDI$predicted <- predict(lm(post~pre, BDI))\n  mean(BDI$predicted)\n\n[1] 22.95114\n\n  var(BDI$predicted)\n\n[1] 95.63817\n\n  # The predicted values have the same mean (so no systematic treatment effect, as expected)\n  # but much smaller variance:\n  \n  BDI_long2 <- pivot_longer(BDI, c(pre, post, predicted), names_to = \"cat\")\n  \n  library(ggplot2)\nggplot(BDI_long2, aes(x=as.factor(cat), y=value)) + \n  ggdist::stat_halfeye(adjust = .5, width = .3, .width = 0, justification = -.3, point_colour = NA) + \n  geom_boxplot(width = .1, outlier.shape = NA) +\n  gghalves::geom_half_point(side = \"l\", range_scale = .4, alpha = .5)\n\n\n\n\nAssumed that we use the predicted scores at t2 (post) and predict the next scores at t3 - will it shrink ever more, until all data points are at the mean? But this is not substantively reflected in the raw scores. Is it an artifact of regression?"
  },
  {
    "objectID": "Other_resources.html",
    "href": "Other_resources.html",
    "title": "Other resources",
    "section": "",
    "text": "https://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/ (see Parts I to IV)\n“Simulation-based power analysis for regression” by Andrew Hales (Youtube video, OSF Material)"
  }
]