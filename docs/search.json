[
  {
    "objectID": "GLMM.html",
    "href": "GLMM.html",
    "title": "GLMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "Structural Equation Models (SEM)",
    "section": "",
    "text": "#install.packages(c(\"ggplot2\", \"MASS\", \"apaTables\", \"MBESS))\nlibrary(\"lavaan\")\nlibrary(\"MASS\")\nlibrary(\"tidyverse\")\nlibrary(\"MBESS\")\nIn this chapter, we will focus on a few rather simple Structural Equation Models (SEM). The goal is to illustrate how simulations can be used to estimate statistical power to detect a given effect in a SEM. In the context of SEMs, the focal effect may be for instance a fit index (e.g., Chi-Square) or a model coefficient (e.g., a regression coefficient for the association between two latent factors). We will start with the former, i.e., a power analysis for a fit index."
  },
  {
    "objectID": "SEM.html#lets-get-some-real-data-as-starting-point",
    "href": "SEM.html#lets-get-some-real-data-as-starting-point",
    "title": "Structural Equation Models (SEM)",
    "section": "Let’s get some real data as starting point",
    "text": "Let’s get some real data as starting point\nJust like for any other simulation-based power analysis, we first need to come up with plausible estimates of the distribution of the (manifest) variables. For the sake of simplicity, let’s assume that there is a published study that measured the four manifest variables of interest (i.e., prejudice towards women, foreigners, homosexuals, and disabled people) and that the corresponding data set is publicly available. For the purpose of this tutorial, we will draw on a publication by Bergh et al (2016) and the corresponding data set which has been made accessible as part of the MPsychoR package. Let’s take a look at this data set.\n\n#install.packages(\"MPsychoR\")\nlibrary(MPsychoR)\ndata(\"Bergh\")\n\n#let's take a look\nhead(Bergh)\n\n        EP    SP  HP       DP     A1       A2       A3     O1       O2       O3\n1 2.666667 3.125 1.4 2.818182 3.4375 3.600000 3.352941 2.8750 3.400000 3.176471\n2 2.666667 3.250 1.4 2.545455 2.3125 2.666667 3.117647 4.4375 3.866667 4.470588\n3 1.000000 1.625 2.7 2.000000 3.5625 4.600000 3.941176 4.2500 3.666667 3.705882\n4 2.666667 2.750 1.8 2.818182 2.7500 3.200000 3.352941 2.8750 3.400000 3.117647\n5 2.888889 3.250 2.7 3.000000 3.2500 4.200000 3.764706 3.9375 4.400000 4.294118\n6 2.000000 2.375 1.7 2.181818 3.2500 3.333333 2.941176 3.8125 3.066667 3.411765\n  gender\n1   male\n2   male\n3   male\n4   male\n5   male\n6   male\n\n\nThis data set comprises 11 variables measured in 861 participants. For now, we will focus on four measured variables:\n\nEP is a continuous variable measuring ethnic prejudice.\nSP is a continuous variable measuring sexism.\nHP is a continuous variable measuring sexual prejudice towards gays and lesbians.\nDP is a continuous variable measuring prejudice toward mentally people with disabilities.\n\nTo get an impression of this data, we can inspect means, standard deviations, and correlations of the variables we’re interested in. I am using the attach function which makes it easier to access variables of this data set thourght this tutorial without specifying the data set containing this variable again and again.\n\nattach(Bergh)\napaTables::apa.cor.table(cbind(EP, SP, HP, DP))\n\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M    SD   1          2          3         \n  1. EP    1.99 0.71                                 \n                                                     \n  2. SP    2.11 0.68 .53**                           \n                     [.48, .58]                      \n                                                     \n  3. HP    1.22 1.56 .25**      .22**                \n                     [.19, .32] [.16, .28]           \n                                                     \n  4. DP    2.06 0.53 .53**      .53**      .24**     \n                     [.48, .58] [.48, .57] [.18, .30]\n                                                     \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p < .05. ** indicates p < .01.\n \n\n\nAs we have discussed in the previous chapters, the starting point of every simulation-based power analysis is to specify the population parameters of the variables of interest. With these population parameters, we can then simulate data, for example using the mrvnorm function from the MASS package. In our example, we estimate the population parameters from the study by Bergh et al. (2016). We start by calculating the means of the four variables, rounding them generously, and storing them in a vector called means_vector.\n\n# FIXME: Rather use the base pipe |> ? Do we need dplyr here?\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP)) %>% round(1)\n\nWe also need the variance-covariance matrix of our variables in order to simulate data. Again, we can estimate this from the Bergh et al. data. The following chunk takes the correlation matrix as well as the standard deviations from the Bergh et al. study. These two objects can later be used to estimate the variance-covariance matrix.\n\n#store correlation matrix\ncor_mat <- cor(cbind(EP, SP, HP, DP))\n\n#store standard deviations\nsd_vector <- c(sd(EP), sd(SP), sd(HP), sd(DP))\n\nTo transform these two objects into a variance-covariance matrix, we can use the cor2cov function in the MBESS package. Like before, we round the resulting matrix generously and store it in an object.\n\ncov_mat <- MBESS::cor2cov(cor.mat = cor_mat, sd = sd_vector) %>% as.data.frame() %>% round(1)\n\n# FIXME: Wouldn't it be easier here to simple run cov(cbind(EP, SP, HP, DP))?\n# (on the other hand, it is good to know how to get the var-cov-matrix if only correlations are reported in a paper)\n\nNow that we have an approximation of the variance-covariance matrix, we use the mvrnorm function in the MASS package to simulate data from a multivariate normal distribution. The following code simulates n = 100 observations from the specified population.\n\n#Set seed to make results reproduicble\nset.seed(2134)\n\nsimulated_data <- MASS::mvrnorm(n = 100, mu=means_vector, Sigma = cov_mat) %>% as.data.frame()\n\nhead(simulated_data)\n\n        EP        SP         HP        DP\n1 0.721832 0.7609397  0.7700167 0.2456508\n2 1.635470 2.8519189  1.4963049 2.1757972\n3 1.591969 0.9179549  2.4261705 2.2481749\n4 2.860149 2.0571407  1.5488791 1.6693060\n5 2.945434 3.2491068  1.9372693 3.0506263\n6 1.608101 1.9478716 -0.9639594 1.3189472\n\n\nWe could now fit a SEM to this simulated data set and check whether the Chi-Squared value is significant at an \\(\\alpha\\)-level of .005.\n\nmodel_cfa <- \"generalized_prejudice =~ EP + DP + SP + HP\"\nfit_cfa <- cfa(model_cfa, data = simulated_data)\nsummary(fit_cfa, fit.measures = TRUE)\n\nlavaan 0.6.13 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.641\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.267\n\nModel Test Baseline Model:\n\n  Test statistic                               115.620\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -444.808\n  Loglikelihood unrestricted model (H1)       -443.487\n                                                      \n  Akaike (AIC)                                 905.615\n  Bayesian (BIC)                               926.456\n  Sample-size adjusted Bayesian (SABIC)        901.190\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.215\n  P-value H_0: RMSEA <= 0.050                    0.351\n  P-value H_0: RMSEA >= 0.080                    0.532\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.767    0.120    6.412    0.000\n    SP                        1.195    0.175    6.825    0.000\n    HP                        0.928    0.304    3.051    0.002\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.222    0.047    4.752    0.000\n   .DP                0.185    0.033    5.546    0.000\n   .SP                0.166    0.054    3.056    0.002\n   .HP                2.083    0.302    6.894    0.000\n    generlzd_prjdc    0.302    0.076    3.995    0.000\n\n\nThe results show that in this case, the p-value is 0.267 and thus non-significant. But, actually, its not our primary interest to see whether this particular simulated data set results in an acceptable model fit. Rather, we want to know how many of a theoretically infinite number of simulations yield an acceptable Chi-Squared value. Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size (say, 100 observations) from the specified population and store the results of the focal test (here: the p-value of the Chi-Squared test) in a vector called p_values.\n\n#Let's do 1000 iterations\niterations <- 1000\n\n#Prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\n#Sample size per iteration\nn <- 100\n\n\nfor(i in 1:iterations){\n  \nsimulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) %>% as.data.frame()\nfit_cfa_simulated <- cfa(model_cfa, data = simulated_data)\np_values[i] <-fitMeasures(fit_cfa_simulated)[\"pvalue\"] \n\n}\n\nHow many of our 1000 virtual samples would have found a significant p-value (i.e., p < .005)?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  968    32 \n\n\nOnly 3% of samples with the same size of \\(n=100\\) result in a significant p-value. We conclude that \\(n=100\\) observations seems to be insufficient.\nFIXME: Wait, for the chi2-value we actually “want” to have a nonsignificant p-value - this makes the example a bit difficult. Maybe directly go at another index, such as CFI? (But it must be one that reacts to sample size; I think CFI doesn’t. Maybe the CI of the RMSEA? This gets narrower with increasing n)"
  },
  {
    "objectID": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "SEM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Structural Equation Models (SEM)",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nBut how many observations do we need to find the presumed effect with a power of 80%? Like before, we can now systematically vary some parameters (e.g., sample size) of our simulation and see how that affects power. We could, for example, vary the sample size in a range from 100 to 2000. Running these simulations typically requires quite some time for your computer.\n\nns <- seq(100, 2000, by=100) # test ns between 100 and 2000\n\n#prepare empty vector to store results\nresult <- data.frame()\n\nfor (n in ns){  # outer loop\n  \n  p_values <- rep(NA, iterations)\n  iterations <- 1000\n  \n#prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\nfor(i in 1:iterations){\n  \nsimulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) %>% as.data.frame()\nfit_cfa_simulated <- cfa(model_cfa, data = simulated_data)\np_values[i] <-fitMeasures(fit_cfa_simulated)[\"pvalue\"] \n\n}\n\n\n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n}\n\nLet’s plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line() + scale_y_continuous(n.breaks = 10) + scale_x_continuous(n.breaks = 10)\n\n\n\n\nThis graph shows that we need a sample size of approximately 1250 to reach a power of 80% with the given population estimates."
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "GLM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "optimizing_code.html",
    "href": "optimizing_code.html",
    "title": "Bonus: Optimizing R code for speed",
    "section": "",
    "text": "Code profiling means that the code execution is timed, just like you had a stopwatch. You try to make your code snippet as fast as possible. RStudio has a built-in profiler that (in theory) allows to see which code line takes up the longest time. But in my experience, if the computation of each single line is very short (and the duration mostly comes from the many repetitions), it is very inaccurate (i.e., the time spent is allocated to the wrong lines). Therefore, we’ll resort to the simplest way of timing code: We will measure overall execution time by wrapping our code in a system.time({ ... }) call.\n\nFirst, naive version\nWe use system.time() to measure how long our code execution takes. Longer code blocks need to be wrapped in curly braces {...}. The function returns multiple timings; the relevant number for us is the “user” time.\nHere is a first version of the power simulation code for a simple LM.\n\nt0 <- system.time({\n\niterations <- 2000\nns <- seq(300, 500, by=50)\nresult <- data.frame()\n\nfor (n in ns) {\n  p_values <- c()\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    p_values <- c(p_values, summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"])\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\nt0\n\n   user  system elapsed \n  3.827   0.069   3.901 \n\nresult\n\n    n  power\n1 300 0.3260\n2 350 0.4010\n3 400 0.4605\n4 450 0.5530\n5 500 0.6045\n\n\nThis first version takes 3.827 seconds. Of course we have sampling error here as well; if you run this code multiple times, you will always get slightly different timings. But, again, we refrain from micro-optimizing in the millisecond range, so a single run is generally good enough. You should only tune your simulation in a way that it takes at least a few seconds; if you are in the millisecond range, the timings are imprecise and you won’t see speed improvements very well.\n\n\nRule 1: No growing vectors/data frames\nThis is one of the most common bottlenecks: You start with an empty vector (or even worse: data frame), and grow it by rbind-ing new rows to it in each iteration.\n\nt1 <- system.time({\n\niterations <- 2000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # print(n)   # uncomment to see progress\n  \n  # CHANGE: Preallocate vector with the final size, initialize with NAs\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    treatment <- c(rep(0, n/2), rep(1, n/2))\n    BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment, data=df)\n    \n    # CHANGE: assign resulting p-value to specific slot in vector\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  # Here we stil have a growing data.frame - but as this is only done 5 times, it does not matter.\n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations)) \n}\n\n})\n\n# Combine the different timings in a data frame\ntimings <- rbind(t0, t1) |> data.frame()\n\n# compute the absolute and relativ difference of consecutive rows:\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1])\n\ntimings\n\n   user.self sys.self elapsed user.child sys.child   diff     rel_diff\nt0     3.827    0.069   3.901          0         0     NA           NA\nt1     3.795    0.038   3.835          0         0 -0.032 -0.008361641\n\n\nOK, this didn’t really change anything here. But in general (in particular with data frames) this is worth looking at.\n\n\nRule 2: Avoid data frames as far as possible\nUse matrizes instead of data frames wherever possible; or avoid them at all (as we do in the code below).\n\nt2 <- system.time({\n\niterations <- 2000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    \n    # CHANGE: We don't need the data frame - just create the two variables in the environment and lm() takes them from there.\n    #df <- data.frame(treatment, BDI)\n    res <- lm(BDI ~ treatment)\n    \n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\n\ntimings <- rbind(t0, t1, t2) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1])\ntimings\n\n   user.self sys.self elapsed user.child sys.child   diff     rel_diff\nt0     3.827    0.069   3.901          0         0     NA           NA\nt1     3.795    0.038   3.835          0         0 -0.032 -0.008361641\nt2     2.937    0.030   2.968          0         0 -0.858 -0.226086957\n\n\nThis showed a substantial improvement of around -0.9 seconds; a relative gain of -22.6%.\n\n\nRule 3: Avoid unnecessary computations\nWhat do we actually need? In fact only the p-value for our focal predictor. But the lm function does so many more things, for example parsing the formula BDI ~ treatment.\nWe could strip away all overhead and do only the necessary steps: Fit the linear model, and retrieve the p-values (see https://stackoverflow.com/q/49732933). This needs some deeper knowledge of the functions and some google-fu. When you do this, you should definitely compare your results with the original result from the lm function and verify that they are identical!\n\nt3 <- system.time({\n\niterations <- 2000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    # fit the model:\n    m <- .lm.fit(x, y)\n    \n    # compute p-values based on the residuals:\n    rss <- sum(m$residuals^2)\n    rdf <- length(y) - ncol(x)\n    resvar <- rss/rdf\n    R <- chol2inv(m$qr)\n    se <- sqrt(diag(R) * resvar)\n    ps <- 2*pt(abs(m$coef/se),rdf,lower.tail=FALSE)\n    \n    p_values[i] <- ps[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0, t1, t2, t3) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1])\ntimings\n\n   user.self sys.self elapsed user.child sys.child   diff     rel_diff\nt0     3.827    0.069   3.901          0         0     NA           NA\nt1     3.795    0.038   3.835          0         0 -0.032 -0.008361641\nt2     2.937    0.030   2.968          0         0 -0.858 -0.226086957\nt3     0.307    0.013   0.320          0         0 -2.630 -0.895471570\n\n\n\n\nRule 4: Use optimized packages\nFor many statistical models, there are packages optimized for speed, see for example here: https://stackoverflow.com/q/49732933\n\nlibrary(RcppArmadillo)\n\nt4 <- system.time({\n\niterations <- 2000\nns <- seq(300, 500, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  # construct the design matrix: first column is all-1 (intercept), second column is the treatment factor\n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n\n    # For comparison - do we get the same results? Yes!\n    # res0 <- lm(y ~ x[, 2])\n    # summary(res0)\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\ntimings <- rbind(t0, t1, t2, t3, t4) |> data.frame()\ntimings$diff <- c(NA, timings[2:nrow(timings), 1] - timings[1:(nrow(timings)-1), 1])\ntimings$rel_diff <- c(NA, timings[2:nrow(timings), \"diff\"]/timings[1:(nrow(timings)-1), 1])\ntimings\n\n   user.self sys.self elapsed user.child sys.child   diff     rel_diff\nt0     3.827    0.069   3.901          0         0     NA           NA\nt1     3.795    0.038   3.835          0         0 -0.032 -0.008361641\nt2     2.937    0.030   2.968          0         0 -0.858 -0.226086957\nt3     0.307    0.013   0.320          0         0 -2.630 -0.895471570\nt4     0.288    0.006   0.295          0         0 -0.019 -0.061889251\n\n\nThis step only gave a minor -6% relative increase in speed - but as a bonus, it made our code much easier to read and shorter.\nI think we can stop here - we managed to reduce the execution time for our example from 3.827 seconds to 0.288 seconds - 13.3x faster!\nWith that fast code, we can easily explore a broad parameter range (n ranging from 100 to 1000) and increase the iterations to 2000 for more stable results. (See also: “Bonus: How many Monte-Carlo iterations are necessary?”)\n\n\nShow the code\nt5 <- system.time({\n\niterations <- 2000\nns <- seq(100, 1000, by=50)\n\nresult <- data.frame()\n\nfor (n in ns) {\n\n    x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\n})\nresult\n\n\n      n  power\n1   100 0.0745\n2   150 0.1350\n3   200 0.1790\n4   250 0.2590\n5   300 0.3325\n6   350 0.4285\n7   400 0.4710\n8   450 0.5765\n9   500 0.6190\n10  550 0.6730\n11  600 0.7175\n12  650 0.7595\n13  700 0.8220\n14  750 0.8465\n15  800 0.8650\n16  850 0.9000\n17  900 0.9040\n18  950 0.9185\n19 1000 0.9460\n\n\nSome steps, such as avoiding growing vectors, didn’t really help here, but will help a lot in other scenarios.\nThere are many blog post showing and comparing strategies to increase R performance, e.g.:\n\nhttps://www.r-bloggers.com/2016/01/strategies-to-speedup-r-code/\nhttps://adv-r.hadley.nz/perf-improve.html\nhttps://csgillespie.github.io/efficientR/performance.html\n\n\n\n\n\n\n\nBut always remember:\n\n\n\n“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\nDonald Knuth (Structured Programming with go to Statements, ACM Journal Computing Surveys, Vol 6, No. 4, Dec. 1974. p. 268)\n\n\n\n\nSession Info\nThese speed measurements have been performed on a 2021 MacBook Pro with M1 processor.\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 13.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] RcppArmadillo_0.11.4.2.1 prettycode_1.1.0         colorDF_0.1.7           \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        digest_0.6.31     crayon_1.5.2      lifecycle_1.0.3  \n [5] jsonlite_1.8.4    magrittr_2.0.3    evaluate_0.19     stringi_1.7.8    \n [9] rlang_1.0.6       cli_3.6.0         rstudioapi_0.14   vctrs_0.5.1      \n[13] rmarkdown_2.19    tools_4.2.0       stringr_1.5.0     glue_1.6.2       \n[17] htmlwidgets_1.6.1 purrr_1.0.0       yaml_2.3.6        xfun_0.36        \n[21] fastmap_1.1.0     compiler_4.2.0    htmltools_0.5.4   knitr_1.41"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "check alpha so your models don’t yield more than 5% false-positive results\ncheck beta (power) for easy tests such as t-tests (where this isn’t really needed)\nprepare a preregistration and make sure your code works\ncheck your understanding of statistics\n\nLet’s dive deeper into power calculations for different complex models.\nFor each, we will follow the structure:\n\ndefine what type of data and variables need to be simulated, i.e. their distribution, their class (e.g. factor vs numerical value), sample sizes (within a dataset, and number of replicates), what will need to vary (e.g. the strength of relationship)\ngenerate data, random data or data including an effect (e.g. an imposed correlation between two variables)\nrun the statistical test you think is appropriate, and record the relevant statistic (e.g. p-value)\nreplicate step 2 and 3 to get the distribution of the statistic of interest\ntry out different parameter sets (explore the parameter space for which results are similar)\nanalyze and interpret the combined results of many simulations within each set of parameters. For instance, check that you only get a significant result in 5% of the simulations (if alpha = 0.05) when you simulated no effect; and that you get at a significant result in 80% of the simulations (if you targeted a power of 80%) when you simulated an effect\n\nHere are the type of models we will cover, you can pick and choose what is relevant to you!\n\n[LM](LM.qmd)\nGLM\nLMM\nGLMM\nSEM\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "how_many_iterations.html",
    "href": "how_many_iterations.html",
    "title": "Bonus: How many Monte-Carlo iterations are necessary?",
    "section": "",
    "text": "To find the sample size needed for a study, we have previsouly use e.g. 1000 iterations of data simulation and analysis, and varied the sample size from e.g. 100 to 1000, every 50, to find where the e.g. 80% power threshold was crossed (see e.g. https://shiny.psy.lmu.de/r-tutorials/powersim/LM.html#sample-size-planning-find-the-necessary-sample-size #FIXME relative link). But is 1000 iterations giving a precise enough result?\nUsing the optimized code, we can explore how many Monte-Carlo iterations are necessary to get stable computational results: we can re-run the same simulation with the same sample size!\nLet’s start with 1000 iterations (at n = 100, and 10 repetitions of the same power analysis):\n\nset.seed(0xBEEF)\niterations <- 1000\n\n# CHANGE: do the same sample size repeatedly, and see how much different runs deviate.\nns <- rep(100, 10)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  \n  x <- cbind(\n    rep(1, n),\n    c(rep(0, n/2), rep(1, n/2))\n  )\n  \n  p_values <- rep(NA, iterations)\n  \n  for (i in 1:iterations) {\n    y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n    \n    mdl <- RcppArmadillo::fastLmPure(x, y)\n    pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n    \n    p_values[i] <- pval[2]\n  }\n  \n  result <- rbind(result, data.frame(n = n, power = sum(p_values < .005)/iterations))\n}\n\nresult\n\n     n power\n1  100 0.065\n2  100 0.070\n3  100 0.068\n4  100 0.066\n5  100 0.067\n6  100 0.073\n7  100 0.071\n8  100 0.070\n9  100 0.071\n10 100 0.063\n\n\nAs you can see, the power estimates show some variance, ranging from 0.063 to 0.073. This can be formalized as the Monte-Carlo error (MCE), which is define as “the standard deviation of the Monte Carlo estimator, taken across hypothetical repetitions of the simulation” (Koehler et al., 2009). With 1000 iterations (and 10 repetitions), this is:\n\nsd(result$power) |> round(4)\n\n[1] 0.0031\n\n\nWe only computed 10 repetitions of our power estimate, hence the MCE estimate is quite unstable. In the next computation, we will compute 100 repetitions of each power estimate.\nHow much do we have to increase the iterations to achieve a MCE smaller than, say, 0.005 (i.e, an SD of +/- 0.5% of the power estimate)?\nLet’s loop through increasing iterations (this takes a few minutes):\n\nlibrary(RcppArmadillo)\niterations <- seq(1000, 6000, by=1000)\n\n# let's have 100 iterations to get sufficiently stable MCE estimates\nns <- rep(100, 100)\nresult <- data.frame()\n\nfor (it in iterations) {\n\n  # print(it)  uncomment for showing the progress\n\n  for (n in ns) {\n    \n    x <- cbind(\n      rep(1, n),\n      c(rep(0, n/2), rep(1, n/2))\n    )\n    \n    p_values <- rep(NA, it)\n    \n    for (i in 1:it) {\n      y <- 23 - 3*x[, 2] + rnorm(n, mean=0, sd=sqrt(117))\n      \n      mdl <- RcppArmadillo::fastLmPure(x, y)\n      pval <- 2*pt(abs(mdl$coefficients/mdl$stderr), mdl$df.residual, lower.tail=FALSE)\n      \n      p_values[i] <- pval[2]\n    }\n    \n    result <- rbind(result, data.frame(iterations = it, n = n, power = sum(p_values < .005)/it))\n  }\n}\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(pwr)\n\n# We can compute the exact power with the analytical solution:\nexact_power <- pwr.t.test(d = 3 / sqrt(117), sig.level = 0.005, n = 50)\n\np1 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun.data=mean_cl_normal) + ggtitle(\"Power estimate (error bars = SD)\") + geom_hline(yintercept = exact_power$power, colour = \"blue\", linetype = \"dashed\")\n\np2 <- ggplot(result, aes(x=iterations, y=power)) + stat_summary(fun=\"sd\", geom=\"point\") + ylab(\"MCE\") + ggtitle(\"Monte Carlo Error\")\n\np1/p2\n\n\n\n\nAs you can see, the MCE gets smaller with increasing iterations. The desired precision of MCE <= .005 can be achieved at around 3000 iterations (the dashed blue line is the exact power estimate from the analytical solution). While precision increases quickly by going from 1000 to 2000 iterations, further improvements are costly in terms of computation time. In sum, 3000 iterations seems to be a good compromise for this specific power simulation.\n\n\n\n\n\n\nNote\n\n\n\nThis choice of 3000 iterations does not necessarily generalize to other power simulations with other statistical models. But in my experience, 2000 iterations typically is a good (enough) choice. I often start with 500 iterations when exploring the parameter space (i.e., looking roughly for the range of reasonable sample sizes), and then “zoom” into this range with 2000 iterations.\n\n\nIn the lower plot, you can also see that the MCE estimate itself is a bit wiggly - we would expect a smooth curve. It suffers from meta-MCE! We could increase the precision of the MCE estimate by increasing the number of repetitions (currently at 100)."
  },
  {
    "objectID": "LMM.html",
    "href": "LMM.html",
    "title": "LMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LM.html",
    "href": "LM.html",
    "title": "Linear Model (a single predictor)",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\"))\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM.html#get-some-real-data-as-starting-point",
    "href": "LM.html#get-some-real-data-as-starting-point",
    "title": "Linear Model (a single predictor)",
    "section": "Get some real data as starting point",
    "text": "Get some real data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses - ideally bias-corrected - for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (“Beat the blues”; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a “pilot study” for our power analysis.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0–13: minimal depression\n14–19: mild depression\n20–28: moderate depression\n29–63: severe depression.\n\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample (before #FIXME and after? treatment)?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score in that sample was 23, corresponding to a “moderate depression”.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is \\(17-23=6\\)."
  },
  {
    "objectID": "LM.html#enter-specific-values-for-the-model-parameters",
    "href": "LM.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model (a single predictor)",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet’s rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value #FIXME (i.e. post-2m BDI values) ?:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{2}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value “0” for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. “slope parameter”) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat’s our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample “virtual participants”."
  },
  {
    "objectID": "LM.html#what-is-the-effect-size-in-the-model",
    "href": "LM.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model (a single predictor)",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. The standardized effect size relates the raw effect size to the variability. In the two-group example, this can be expressed as Cohen’s d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen’s d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM.html#doing-the-power-analysis",
    "href": "LM.html#doing-the-power-analysis",
    "title": "Linear Model (a single predictor)",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a loop and repeatedly do the procedure. #FIXME maybe we can use the function ‘replicate’ seen in first tutorial instead of for loop - maybe faster?\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\n# (we do this outside the loop as it stays constant)\niterations <- 1000\nn <- 100\ntreatment <- c(rep(0, n/2), rep(1, n/2))\n\n# vector that collects the p-values from all 1000 virtual samples\n# (prepare an empty NA vector with 1000 slots)\np_values <- rep(NA, iterations)\n\n# now repeatedly draw samples, analyze, and the p-value of the \n# focal regression weight (i.e., the slope parameter)\nfor (i in 1:iterations) {\n  BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  res <- lm(BDI ~ treatment)\n  p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n}\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen’s \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model (a single predictor)",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into another loop that continuously increases the n. We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # outer loop\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value\n  for (i in 1:iterations) {  # inner loop\n    BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet’s plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\n🥳 Congratulations! You did your first power analysis by simulation. 🎉\nFor these simple models, we can also compute analytic solutions. Let’s verify our results with the pwr package:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew 😅"
  },
  {
    "objectID": "LM.html#safeguard-power-analysis",
    "href": "LM.html#safeguard-power-analysis",
    "title": "Linear Model (a single predictor)",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis, that aims for the lower end of a two-sided 60% CI around the parameter of the focal treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the “divide-by-2” heuristic. (TODO: Link to presentation)\n\n\nWe can use the t.test function to compute a CI around that value:\n\nt.test(BtheB$bdi.2m, BtheB$bdi.pre, na.rm=TRUE, conf.level = 0.60)\n\n\n    Welch Two Sample t-test\n\ndata:  BtheB$bdi.2m and BtheB$bdi.pre\nt = -4.1613, df = 194.87, p-value = 4.745e-05\nalternative hypothesis: true difference in means is not equal to 0\n60 percent confidence interval:\n -7.712244 -5.112704\nsample estimates:\nmean of x mean of y \n 16.91753  23.33000 \n\n\n\n\n\n\n\n\nMOTE Online Calculator\n\n\n\nYou can also use the MOTE online calculator by Erin Buchanan et al. to compute effect sizes and confidence intervals around them: https://doomlab.shinyapps.io/mote/\nThis takes either raw statistics (means, SDs) or test statistics as input.\n\n\nAs the assumed effect is negative, we aim for the upper, i.e., the more conservative limit, which is considerably smaller, at -5.1.\n\n\n\n\n\n\nTip: Computing CIs around standardized effect sizes\n\n\n\nWhen you use a standardized mean difference that has been reported in a paper, you can use a function from the MBESS package to compute the CI:\nlibrary(MBESS)\n\n# smd = standardized mean difference = Cohen's d\nci.smd(smd=0.55, n.1=50, n.2=50, conf.level=0.60)\n\n\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -5.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 5.1*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  100 0.314\n2  120 0.408\n3  140 0.473\n4  160 0.570\n5  180 0.618\n6  200 0.717\n7  220 0.735\n8  240 0.795\n9  260 0.823\n10 280 0.878\n11 300 0.887\n\n\nWith that more conservative effect size assumption, we would need around 240 participants, i.e. 120 per group."
  },
  {
    "objectID": "LM.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model (a single predictor)",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effects size, but rather for the smallest effect size of interest. In this case, a non-significant result can be interpreted as “We accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant”.\nWhat change of BDI scores is perceived as “clinically important”? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al. (2015) analyzed data sets where patients have been asked, after a treatment, whether they felt “better”, “the same” or “worse”. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms where measureably reduced in the BDI, patients still might answer “feels the same”, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. For example, the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nLet’s use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible and suggests that this is simply Monte Carlo sampling error - 1000 iterations is not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\n\n# To speed up computations, we limited the range of ns to the relevant region\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n    n  power\n1 640 0.7583\n2 660 0.7758\n3 680 0.7865\n4 700 0.8060\n5 720 0.8192\n6 740 0.8273\n\n\nNow the power increases monotonically with sample size, as expected."
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  }
]