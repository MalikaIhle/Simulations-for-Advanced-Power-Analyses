{
  "hash": "64d1734de43db1b0533ad47034f624a0",
  "result": {
    "markdown": "---\ntitle: \"Structural Equation Models (SEM)\"\nformat: html\nauthor: Moritz Fischer\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(c(\"ggplot2\", \"MASS\", \"apaTables\", \"MBESS))\nlibrary(\"lavaan\")\nlibrary(\"MASS\")\nlibrary(\"tidyverse\")\nlibrary(\"MBESS\")\n```\n:::\n\n\nIn this chapter, we will focus on a few rather simple Structural Equation Models (SEM). The goal is to illustrate how simulations can be used to estimate statistical power to detect a given effect in a SEM. In the context of SEMs, the focal effect may be for instance a fit index (e.g., Chi-Square) or a model coefficient (e.g., a regression coefficient for the association between two latent factors). We will start with the former, i.e., a power analysis for a fit index.\n\n# A simulation-based power analysis for a fit index in SEM\n\nLet's consider the following example: We are planning to conduct a study investigating whether prejudice towards different social groups (e.g., women, foreigners, homosexuals, disabled people) can be conceptualized as manifestations of a single underlying latent factor (aka \"generalized prejudice\"). In other words, we aim to test whether a SEM in which four manifest variables load on a single higher factor (\"generalized prejudice\") has an adequate fit. There are multiple fit indices which we could use to decide whether this model has an acceptable fit, such as Chi-Squared, CFI, SRMR, or RMSEA. We will not go into detail of the pros and cons of these indices here. Let's for now just focus on one of these indices: The Chi-Squared statistic. \n\n## Let's get some real data as starting point\n\nJust like for any other simulation-based power analysis, we first need to come up with plausible estimates of the distribution of the (manifest) variables. For the sake of simplicity, let's assume that there is a published study that measured the four manifest variables of interest (i.e., prejudice towards women, foreigners, homosexuals, and disabled people) and that the corresponding data set is publicly available. For the purpose of this tutorial, we will draw on a publication by Bergh et al (2016) and the corresponding data set which has been made accessible as part of the `MPsychoR` package. Let's take a look at this data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"MPsychoR\")\nlibrary(MPsychoR)\ndata(\"Bergh\")\n\n#let's take a look\nhead(Bergh)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        EP    SP  HP       DP     A1       A2       A3     O1       O2       O3\n1 2.666667 3.125 1.4 2.818182 3.4375 3.600000 3.352941 2.8750 3.400000 3.176471\n2 2.666667 3.250 1.4 2.545455 2.3125 2.666667 3.117647 4.4375 3.866667 4.470588\n3 1.000000 1.625 2.7 2.000000 3.5625 4.600000 3.941176 4.2500 3.666667 3.705882\n4 2.666667 2.750 1.8 2.818182 2.7500 3.200000 3.352941 2.8750 3.400000 3.117647\n5 2.888889 3.250 2.7 3.000000 3.2500 4.200000 3.764706 3.9375 4.400000 4.294118\n6 2.000000 2.375 1.7 2.181818 3.2500 3.333333 2.941176 3.8125 3.066667 3.411765\n  gender\n1   male\n2   male\n3   male\n4   male\n5   male\n6   male\n```\n:::\n:::\n\n\nThis data set comprises 11 variables measured in 861 participants. For now, we will focus on four measured variables: \n\n-   `EP` is a continuous variable measuring ethnic prejudice. \n-   `SP` is a continuous variable measuring sexism. \n-   `HP` is a continuous variable measuring sexual prejudice towards gays and lesbians. \n-   `DP` is a continuous variable measuring prejudice toward mentally people with disabilities. \n\nTo get an impression of this data, we can inspect means, standard deviations, and correlations of \nthe variables we're interested in. I am using the `attach` function which makes it easier to access variables of this data set thourght this tutorial without specifying the data set containing this variable again and again. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Bergh)\napaTables::apa.cor.table(cbind(EP, SP, HP, DP))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M    SD   1          2          3         \n  1. EP    1.99 0.71                                 \n                                                     \n  2. SP    2.11 0.68 .53**                           \n                     [.48, .58]                      \n                                                     \n  3. HP    1.22 1.56 .25**      .22**                \n                     [.19, .32] [.16, .28]           \n                                                     \n  4. DP    2.06 0.53 .53**      .53**      .24**     \n                     [.48, .58] [.48, .57] [.18, .30]\n                                                     \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p < .05. ** indicates p < .01.\n \n```\n:::\n:::\n\n\nAs we have discussed in the previous chapters, the starting point of every simulation-based power analysis is to specify the population parameters of the variables of interest. With these population parameters, we can then simulate data, for example using the `mrvnorm` function from the `MASS` package. In our example, we estimate the population parameters from the study by Bergh et al. (2016). We start by calculating the means of the four variables, rounding them generously, and storing them in a vector called `means_vector`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# FIXME: Rather use the base pipe |> ? Do we need dplyr here?\nmeans_vector <- c(mean(EP), mean(SP), mean(HP), mean(DP)) %>% round(1)\n```\n:::\n\n\nWe also need the variance-covariance matrix of our variables in order to simulate data. Again, we can estimate this from the Bergh et al. data. The following chunk takes the correlation matrix as well as the standard deviations from the Bergh et al. study. These two objects can later be used to estimate the variance-covariance matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#store correlation matrix\ncor_mat <- cor(cbind(EP, SP, HP, DP))\n\n#store standard deviations\nsd_vector <- c(sd(EP), sd(SP), sd(HP), sd(DP))\n```\n:::\n\n\nTo transform these two objects into a variance-covariance matrix, we can use the `cor2cov` function in the `MBESS` package. Like before, we round the resulting matrix generously and store it in an object. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov_mat <- MBESS::cor2cov(cor.mat = cor_mat, sd = sd_vector) %>% as.data.frame() %>% round(1)\n\n# FIXME: Wouldn't it be easier here to simple run cov(cbind(EP, SP, HP, DP))?\n# (on the other hand, it is good to know how to get the var-cov-matrix if only correlations are reported in a paper)\n```\n:::\n\n\nNow that we have an approximation of the variance-covariance matrix, we use the `mvrnorm` function in the `MASS` package to simulate data from a multivariate normal distribution. The following code simulates `n = 100` observations from the specified population. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Set seed to make results reproduicble\nset.seed(2134)\n\nsimulated_data <- MASS::mvrnorm(n = 100, mu=means_vector, Sigma = cov_mat) %>% as.data.frame()\n\nhead(simulated_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        EP        SP         HP        DP\n1 0.721832 0.7609397  0.7700167 0.2456508\n2 1.635470 2.8519189  1.4963049 2.1757972\n3 1.591969 0.9179549  2.4261705 2.2481749\n4 2.860149 2.0571407  1.5488791 1.6693060\n5 2.945434 3.2491068  1.9372693 3.0506263\n6 1.608101 1.9478716 -0.9639594 1.3189472\n```\n:::\n:::\n\n\nWe could now fit a SEM to this simulated data set and check whether the Chi-Squared value is significant at an $\\alpha$-level of .005.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_cfa <- \"generalized_prejudice =~ EP + DP + SP + HP\"\nfit_cfa <- cfa(model_cfa, data = simulated_data)\nsummary(fit_cfa, fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.13 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.641\n  Degrees of freedom                                 2\n  P-value (Chi-square)                           0.267\n\nModel Test Baseline Model:\n\n  Test statistic                               115.620\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994\n  Tucker-Lewis Index (TLI)                       0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -444.808\n  Loglikelihood unrestricted model (H1)       -443.487\n                                                      \n  Akaike (AIC)                                 905.615\n  Bayesian (BIC)                               926.456\n  Sample-size adjusted Bayesian (SABIC)        901.190\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.215\n  P-value H_0: RMSEA <= 0.050                    0.351\n  P-value H_0: RMSEA >= 0.080                    0.532\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.767    0.120    6.412    0.000\n    SP                        1.195    0.175    6.825    0.000\n    HP                        0.928    0.304    3.051    0.002\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.222    0.047    4.752    0.000\n   .DP                0.185    0.033    5.546    0.000\n   .SP                0.166    0.054    3.056    0.002\n   .HP                2.083    0.302    6.894    0.000\n    generlzd_prjdc    0.302    0.076    3.995    0.000\n```\n:::\n:::\n\n\nThe results show that in this case, the p-value is 0.267 and thus non-significant. But, actually, its not our primary interest to see whether this particular simulated data set results in an acceptable model fit. Rather, we want to know how many of a theoretically infinite number of simulations yield an acceptable Chi-Squared value. Thus, as in the previous chapters, we now repeatedly simulate data sets of a certain size (say, 100 observations) from the specified population and store the results of the focal test (here: the p-value of the Chi-Squared test) in a vector called `p_values`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Let's do 1000 iterations\niterations <- 1000\n\n#Prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\n#Sample size per iteration\nn <- 100\n\n\nfor(i in 1:iterations){\n  \nsimulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) %>% as.data.frame()\nfit_cfa_simulated <- cfa(model_cfa, data = simulated_data)\np_values[i] <-fitMeasures(fit_cfa_simulated)[\"pvalue\"] \n\n}\n```\n:::\n\n\nHow many of our 1000 virtual samples would have found a significant p-value (i.e., p < .005)?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(p_values < .005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n  968    32 \n```\n:::\n:::\n\n\nOnly 3% of samples with the same size of $n=100$ result in a significant p-value. We conclude that $n=100$ observations seems to be insufficient. \n\nFIXME: Wait, for the chi2-value we actually \"want\" to have a nonsignificant  p-value - this makes the example a bit difficult. Maybe directly go at another index, such as CFI?\n(But it must be one that reacts to sample size; I think CFI doesn't. Maybe the CI of the RMSEA? This gets narrower with increasing n)\n\n## Sample size planning: Find the necessary sample size\n\nBut how many observations do we need to find the presumed effect with a power of 80%? Like before, we can now systematically vary some parameters (e.g., sample size) of our simulation and see how that affects power. We could, for example, vary the sample size in a range from 100 to 2000. Running these simulations typically requires quite some time for your computer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- seq(100, 2000, by=100) # test ns between 100 and 2000\n\n#prepare empty vector to store results\nresult <- data.frame()\n\nfor (n in ns){  # outer loop\n  \n  p_values <- rep(NA, iterations)\n  iterations <- 1000\n  \n#prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\nfor(i in 1:iterations){\n  \nsimulated_data <- MASS::mvrnorm(n = n, mu = means_vector, Sigma = cov_mat) %>% as.data.frame()\nfit_cfa_simulated <- cfa(model_cfa, data = simulated_data)\np_values[i] <-fitMeasures(fit_cfa_simulated)[\"pvalue\"] \n\n}\n\n\n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n}\n```\n:::\n\n\nLet's plot there result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line() + scale_y_continuous(n.breaks = 10) + scale_x_continuous(n.breaks = 10)\n```\n\n::: {.cell-output-display}\n![](SEM_files/figure-html/plot cfa-1.png){width=672}\n:::\n:::\n\n\nThis graph shows that we need a sample size of approximately 1250 to reach a power of 80% with the given population estimates. \n\n# A simulation-based power analysis for a regression coefficient in SEM\n\nIn many practical instances, we are, however, not so much interested in whether or not the Chi-Square statistic is significant, but rather whether a single model coefficient (e.g., a regression coefficient modelling the association between two latent factors) is significantly different from zero. We now turn to a simulation-based power analysis for this case. Let's first extend our previous model a bit in order to incorporate more than one latent factor. Luckily, the `Bergh` data set also contains other variables, for instance three items measuring Agreeableness (i.e., `A1`. `A2`, `A3`). Assume that we hypothesized that the the latent Agreeableness factor is negatively related to the (latent) generalized prejudice factor. We can test this with the `Bergh` data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_sem <- \"generalized_prejudice =~ EP + DP + SP + HP\n              agreeableness =~ A1 + A2 + A3\n              generalized_prejudice ~ agreeableness \n              \"\nfit_sem <- cfa(model_sem, data = Bergh)\nsummary(fit_sem, fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.13 ended normally after 42 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           861\n\nModel Test User Model:\n                                                      \n  Test statistic                               111.747\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2590.325\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.962\n  Tucker-Lewis Index (TLI)                       0.938\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4664.837\n  Loglikelihood unrestricted model (H1)      -4608.964\n                                                      \n  Akaike (AIC)                                9359.675\n  Bayesian (BIC)                              9431.046\n  Sample-size adjusted Bayesian (SABIC)       9383.410\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.094\n  90 Percent confidence interval - lower         0.078\n  90 Percent confidence interval - upper         0.110\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    0.930\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.051\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                           Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice =~                                    \n    EP                        1.000                           \n    DP                        0.790    0.045   17.446    0.000\n    SP                        0.908    0.055   16.613    0.000\n    HP                        0.974    0.117    8.329    0.000\n  agreeableness =~                                            \n    A1                        1.000                           \n    A2                        0.909    0.032   28.818    0.000\n    A3                        1.029    0.032   31.927    0.000\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(>|z|)\n  generalized_prejudice ~                                    \n    agreeableness           -0.594    0.051  -11.707    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .EP                0.237    0.017   14.144    0.000\n   .DP                0.118    0.009   12.492    0.000\n   .SP                0.248    0.016   15.622    0.000\n   .HP                2.181    0.108   20.114    0.000\n   .A1                0.072    0.005   14.415    0.000\n   .A2                0.072    0.005   15.716    0.000\n   .A3                0.038    0.004    9.175    0.000\n   .generlzd_prjdc    0.203    0.020   10.330    0.000\n    agreeableness     0.182    0.012   14.841    0.000\n```\n:::\n:::\n\n\nIn order to plan the sample size for a (replication) study, we could like before use the means, standard deviations, and correlations of this data set in order to simulate data. We therefore again store the means of the focal variables in a vector, store the standard deviations in a vector, and transform the correlations into a variance-covariance matrix. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Store means, standard deviations, correlations\nmeans_vector_sem <- c(mean(EP), mean(SP), mean(HP), mean(DP), mean(A1), mean(A2), mean(A3)) %>% round(1)\ncor_mat_sem <- cor(cbind(EP, SP, HP, DP, A1, A2, A3)) %>% round(1)\nsd_vector_sem <- c(sd(EP), sd(SP), sd(HP), sd(DP), sd(A1), sd(A2), sd(A3)) %>% round(1)\n\n#Transform into a variance-covariance matrix\ncov_mat_sem <- MBESS::cor2cov(cor.mat = cor_mat_sem, sd = sd_vector_sem)\n```\n:::\n\n\nWith these estimates, we can simulate a data set of for instance 500 observations. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_data_sem <- MASS::mvrnorm(n = 500, mu = means_vector_sem, Sigma = cov_mat_sem) %>% as.data.frame()\n\nhead(simulated_data_sem)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         EP       SP         HP       DP       A1       A2       A3\n1 1.9964210 2.523573  0.6768617 2.163980 3.048106 2.937798 3.266207\n2 1.9202970 1.737811 -0.7673963 1.076614 4.332241 4.310267 4.552578\n3 1.3697006 2.056787  1.9890652 2.588360 2.925629 2.862049 3.207422\n4 1.6745218 1.961145  1.9419870 1.951226 2.119809 2.959966 2.744540\n5 2.4267444 2.140284  1.3833893 2.364990 3.437155 3.361423 3.549565\n6 0.6369288 1.377539 -0.9879865 0.385836 4.325369 4.842550 4.400937\n```\n:::\n:::\n\n\nReplicating the previous power analysis, we now wrap this simulation in a loop while varying the sample size and storing the observed power. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- seq(60, 200, by=10) # test ns between 60 and 200\n\n#prepare empty vector to store results\nresult_sem <- data.frame()\n\nfor (n in ns){  # outer loop\n  \n  p_values <- rep(NA, iterations)\n  iterations <- 1000\n  \n#prepare an empty NA vector with 1000 slots\np_values <- rep(NA, iterations)\n\nfor(i in 1:iterations){\n  \nsimulated_data_sem <- MASS::mvrnorm(n = n, mu=means_vector_sem, Sigma = cov_mat_sem) %>% as.data.frame()\nfit_cfa_simulated <- cfa(model_sem, data = simulated_data_sem)\nparameter_sem <-parameterEstimates(fit_cfa_simulated) %>% filter(lhs == \"generalized_prejudice\", op == \"~\", rhs == \"agreeableness\")\np_values[i] <- parameter_sem[1, \"pvalue\"]\n\n}\n\n\n  result_sem <- rbind(result_sem, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n}\n```\n:::\n\n\nLet's plot this again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(result_sem, aes(x=n, y=power)) + geom_point() + geom_line() + scale_y_continuous(n.breaks = 10) + scale_x_continuous(n.breaks = 10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](SEM_files/figure-html/plot sem-1.png){width=672}\n:::\n:::\n\n\nThis simulation suggests that approx. 83 participants are needed to obtain 80% power. \n\n# References\n\nBergh, R., Akrami, N., Sidanius, J., & Sibley, C. G. (2016). Is group membership necessary for understanding generalized prejudice? A re-evaluation of why prejudices are interrelated. Journal of Personality and Social Psychology, 111(3), 367–395. https://doi.org/10.1037/pspi0000064\n\nWang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 1–17. https://doi.org/10.1177/2515245920918253\n\n",
    "supporting": [
      "SEM_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}